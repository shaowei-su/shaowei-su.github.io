{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [digits.data.shape[1], 16, 16, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 16, 16, 10]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [np.random.randn(x, y) for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.45925885,  0.48421544, -0.7880417 , ..., -0.48558135,\n",
       "         -0.09058042, -1.4996869 ],\n",
       "        [ 0.08556095, -0.66597537, -0.29572179, ...,  0.1482121 ,\n",
       "         -1.19252687, -1.27271259],\n",
       "        [-1.13761115, -0.33863198, -0.5010875 , ..., -0.25252456,\n",
       "          1.97827172,  1.24986115],\n",
       "        ...,\n",
       "        [ 0.51451048,  1.38107286,  0.86944066, ..., -0.07886087,\n",
       "          0.98088437,  0.00639155],\n",
       "        [-0.07713306, -0.29766385,  1.82196367, ..., -0.46005961,\n",
       "          0.1687411 , -0.14741362],\n",
       "        [ 1.67562995, -0.81973236,  0.79643835, ...,  0.05247948,\n",
       "         -1.73647545,  0.64169606]]),\n",
       " array([[-4.01951641e-01, -2.21780819e+00,  3.81192034e-02,\n",
       "         -1.03615474e+00,  6.87094031e-02,  1.10405985e+00,\n",
       "          1.01025273e+00,  4.70212228e-01,  4.06509717e-01,\n",
       "          1.30471129e+00,  8.24139027e-01,  5.51574833e-01,\n",
       "         -1.18191248e+00, -1.47566464e-01,  1.05039944e-01,\n",
       "          2.91338300e+00],\n",
       "        [ 1.05241404e+00, -4.99741108e-01,  8.57714752e-01,\n",
       "         -1.18720651e-01,  2.25215612e+00,  1.07823834e+00,\n",
       "         -9.92406163e-01,  2.31875740e-01, -9.21026057e-02,\n",
       "         -2.75330568e-01,  8.64558320e-01,  4.24524988e-01,\n",
       "          6.30208895e-02,  1.75867470e-01,  2.47563657e-01,\n",
       "         -1.01231624e+00],\n",
       "        [ 1.18914463e+00,  1.25908710e+00,  1.12255978e+00,\n",
       "          5.90757580e-01, -2.62113158e-01,  9.57702014e-01,\n",
       "         -4.70445836e-01, -1.70725289e-01,  1.35035978e-01,\n",
       "          2.73792690e-01,  1.38073277e-03, -1.03239913e+00,\n",
       "          1.05819066e+00,  2.57751707e-02, -3.64112329e-01,\n",
       "         -5.77701419e-02],\n",
       "        [-5.97947934e-01, -6.47093928e-01, -1.03764430e+00,\n",
       "         -9.24884816e-01, -1.25211645e-01, -1.01044138e+00,\n",
       "          1.91038836e-02,  5.68364769e-01,  3.66280582e-01,\n",
       "          1.17466348e+00,  8.21352311e-01, -4.97165360e-01,\n",
       "         -2.30643924e+00,  5.13234912e-01,  6.84689526e-01,\n",
       "          1.65182144e+00],\n",
       "        [ 6.03046453e-01, -8.00888807e-01, -6.05960039e-01,\n",
       "         -9.26221229e-01,  1.12165148e+00, -4.60152963e-01,\n",
       "          1.19564674e+00,  4.07952455e-01, -1.79744266e+00,\n",
       "         -8.36581390e-01, -4.69517859e-01, -1.87210204e+00,\n",
       "         -8.50353516e-01, -1.08967908e+00,  5.35167029e-01,\n",
       "         -1.46081502e+00],\n",
       "        [-1.85298652e+00, -3.83340675e-01, -9.44380752e-01,\n",
       "         -6.96505793e-01, -2.43320340e-01, -8.62389656e-01,\n",
       "          1.27451633e+00, -1.59100514e+00, -1.60832641e-01,\n",
       "         -5.89812601e-01, -5.49145073e-01,  5.99626623e-01,\n",
       "          1.81780890e+00, -3.65994438e-02, -1.30210191e-01,\n",
       "          2.92552758e-01],\n",
       "        [ 3.54795009e-01, -1.41188685e+00,  7.83121159e-01,\n",
       "         -3.80631700e-02, -7.54457171e-01, -1.24579543e-01,\n",
       "          1.05623066e+00,  1.56154641e+00,  1.07824195e+00,\n",
       "         -7.23707949e-01,  1.18100281e+00, -1.42994412e+00,\n",
       "          1.66019801e+00, -9.04143219e-01,  1.40304981e+00,\n",
       "          2.87425312e-01],\n",
       "        [ 8.04377801e-01,  2.96003918e-01, -6.25213919e-01,\n",
       "         -8.05205770e-01,  4.85924592e-01, -8.60062753e-02,\n",
       "         -8.16118519e-02, -1.57951471e-02, -1.34101714e+00,\n",
       "         -7.42433137e-01, -1.44231576e-01, -1.02256097e+00,\n",
       "         -1.58052760e+00, -6.74963281e-01, -2.98487064e-03,\n",
       "          1.97046414e+00],\n",
       "        [ 2.93100190e-01, -1.62761627e+00,  1.58083000e-01,\n",
       "          9.04570380e-01, -8.81562104e-01, -8.55932087e-01,\n",
       "         -2.03938787e-01, -1.56397328e-01,  1.22940028e-01,\n",
       "         -1.14910243e+00, -1.26375940e+00, -2.62882220e-01,\n",
       "         -1.07011649e+00, -1.41985863e+00,  1.44911822e-01,\n",
       "          5.04587177e-01],\n",
       "        [ 7.58040088e-01, -1.85835262e-01,  2.68328882e-01,\n",
       "          2.15250298e+00,  9.19813082e-01,  1.02150944e+00,\n",
       "         -3.10235971e-01, -7.04985665e-01, -3.42714543e-01,\n",
       "          2.77658464e-01, -5.00864433e-01,  1.00880128e+00,\n",
       "         -6.09903809e-01,  2.13481886e-01,  5.10129271e-01,\n",
       "          8.65118635e-01],\n",
       "        [ 8.21220679e-01, -1.83573656e+00, -1.17949920e+00,\n",
       "          1.28074907e+00, -2.05218151e+00,  1.15140096e+00,\n",
       "          1.58982492e+00,  7.96527892e-01, -3.94883201e-01,\n",
       "         -2.34081149e-01, -7.35274619e-01, -1.00060515e+00,\n",
       "         -1.04914633e-01, -3.81729606e-01,  6.46030508e-02,\n",
       "          2.23600964e+00],\n",
       "        [-8.47631598e-01, -1.46999861e+00,  1.29735018e+00,\n",
       "          1.73945347e+00, -3.48532328e-01, -7.02553462e-01,\n",
       "          1.47954419e-01,  6.31860070e-02, -2.22258445e-02,\n",
       "         -1.35265327e+00,  1.52022577e-01,  7.42536675e-01,\n",
       "         -2.38784694e-01, -2.67310938e-03,  1.42121839e-01,\n",
       "         -1.35779787e+00],\n",
       "        [ 9.62336214e-01,  1.88383957e+00,  2.61379223e-02,\n",
       "         -4.17882548e-01, -1.62849052e-01,  5.29281391e-01,\n",
       "          1.26009970e+00,  1.13540448e+00, -2.45015979e-01,\n",
       "          1.04288408e+00, -4.86811449e-01,  1.56805422e-01,\n",
       "         -6.10583195e-01, -1.29306884e+00, -2.59696320e+00,\n",
       "         -1.21976833e+00],\n",
       "        [-1.33823562e+00,  9.44492182e-01,  2.91429331e-01,\n",
       "         -7.48608827e-01,  2.98079265e-02, -1.80094716e-01,\n",
       "          3.94004362e-01, -1.18490340e-01,  1.10251173e+00,\n",
       "          8.02930480e-01,  1.13922456e-01, -8.52181355e-01,\n",
       "         -8.25589626e-01, -7.04188104e-01, -1.38031923e+00,\n",
       "         -1.49054462e+00],\n",
       "        [ 3.43473814e-01,  1.44751652e-01, -1.34381761e+00,\n",
       "          8.95436830e-01,  4.16037185e-01,  8.23453340e-01,\n",
       "         -1.34376759e+00,  2.66686120e-01,  7.22200508e-01,\n",
       "         -6.21778709e-01, -2.10400375e+00,  5.81370921e-01,\n",
       "         -1.95196512e-01,  4.41158172e-01,  1.07981589e+00,\n",
       "          4.01827089e-02],\n",
       "        [ 9.33822133e-01,  5.16267114e-01,  4.59369941e-03,\n",
       "          3.82076082e-01,  5.43067715e-01,  5.78687353e-01,\n",
       "         -1.76996597e-01, -1.37873429e+00,  4.49593551e-01,\n",
       "          9.13011502e-01, -5.73212493e-01,  8.41459459e-01,\n",
       "         -1.36438741e+00, -2.48474204e+00, -2.30870729e-01,\n",
       "          4.07603895e-01]]),\n",
       " array([[ 0.6031528 ,  0.20270211, -0.97064646,  0.42472548, -1.79322878,\n",
       "         -1.8160559 , -0.30066184, -0.30118577, -0.61618395,  1.29215407],\n",
       "        [-0.07553933, -1.13004065,  1.57795061, -0.24185519,  1.06010478,\n",
       "         -0.13289612, -0.07841   , -1.01829921, -0.7703457 ,  0.25292918],\n",
       "        [ 1.21914099, -0.51069523, -1.09283162,  0.48790128, -0.77349278,\n",
       "          0.74607223,  0.26773597, -0.03902536,  0.07976692,  0.69883129],\n",
       "        [-0.10809154, -0.8704959 ,  0.76188429, -0.53640853,  0.94899035,\n",
       "         -0.78494491,  0.91429672,  0.62794085, -0.78464163, -0.25259533],\n",
       "        [ 0.10271328,  1.41331448,  0.90287939,  0.47618284, -1.10667975,\n",
       "         -1.91188111,  0.25831799,  1.99534671, -0.22304588,  0.96078486],\n",
       "        [-2.16730208, -0.04944701,  0.86219186, -0.8733331 ,  1.26049988,\n",
       "          0.22339833,  0.04839102, -0.53602349,  0.33650428,  1.75588809],\n",
       "        [-0.22882481,  0.1820973 , -1.45753621, -1.5531896 , -0.19454466,\n",
       "          0.15484793,  0.05470653,  0.41824079,  0.39201728,  0.50865751],\n",
       "        [ 0.23101259,  1.83568651,  1.80769688,  0.65201424, -0.25383791,\n",
       "         -0.15742945, -2.452997  ,  0.74970363,  0.36094383, -0.50155523],\n",
       "        [-0.3587199 ,  0.05767092,  0.12065885, -0.17561662,  0.562865  ,\n",
       "          0.55761831,  0.67045099,  0.97282429, -1.85131019, -0.72358675],\n",
       "        [-1.09196671, -0.58702688,  0.09283834, -1.71222278,  0.73398255,\n",
       "         -1.40519051, -0.53589078,  1.61817294, -0.65007927,  0.02043916],\n",
       "        [ 0.1074037 , -0.04747975, -0.18155485,  2.92701581,  1.41527416,\n",
       "          2.65968904,  0.89255559, -0.61693353, -0.99733779,  1.01922011],\n",
       "        [ 0.85406792,  1.78211144, -0.62005715,  0.28126392,  1.34855815,\n",
       "          0.44077531,  0.18831386,  0.6415296 ,  1.58049552,  0.78099365],\n",
       "        [-1.30368207, -0.43349485, -0.33053331,  1.05397944, -0.1213584 ,\n",
       "         -0.42861292,  0.32548109,  0.13283194,  0.18869133, -0.42329752],\n",
       "        [ 0.47841912, -0.22919153, -0.45727217, -0.92964239, -0.11628427,\n",
       "         -1.07235438, -0.47427623,  0.47183911,  1.75330586, -0.28064254],\n",
       "        [-1.10130064, -0.18444673, -0.55891533, -1.33827479,  0.45569205,\n",
       "          1.76326238, -0.56831921,  0.84288789,  1.17696536, -0.15697326],\n",
       "        [-0.79439194, -0.80460996,  0.59781375, -0.36422849,  0.5748085 ,\n",
       "          1.55065177,  0.82861403,  1.10808637, -1.10431929, -0.32992765]])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = [np.random.randn(1, y) for y in sizes[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(a, weights, biases):\n",
    "    for i in range(len(weights) - 1):\n",
    "        a = sigmoid(np.dot(a, weights[i]) + biases[i])\n",
    "    ez = np.exp(np.dot(a, weights[-1]) + biases[-1])\n",
    "    a = ez / ez.sum(axis = 1, keepdims = True)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = feed_forward(digits.data, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.34224373e-02, 1.66534785e-02, 1.16458412e-02, 1.01351772e-03,\n",
       "       1.53620981e-03, 4.23515043e-04, 2.61058536e-03, 9.30857464e-01,\n",
       "       3.21393999e-03, 8.62301138e-03])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.zeros((digits.data.shape[0], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[np.arange(digits.data.shape[0]), digits.target] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = yhat - Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = [np.zeros(w.shape) for w in weights]\n",
    "gb = [np.zeros(b.shape) for b in biases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2)\n",
      "(1500, 2)\n",
      "[ 0.55580968 -1.12677442]\n",
      "(1500,)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "### copy paste\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def classification_rate(y, yHat):\n",
    "    n_correct = 0\n",
    "    for i in range(len(y)):\n",
    "        if (y[i] == yHat[i]):\n",
    "            n_correct += 1\n",
    "    crate = float(n_correct)/len(y)\n",
    "    return crate\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(1, y) for y in sizes[1:]]\n",
    "        self.weights =[np.random.randn(x, y) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        for w,b in zip(self.weights, self.biases):\n",
    "            a = sigmoid(np.dot(a,w) + b)\n",
    "        return a\n",
    "\n",
    "    def feedforward_c(self, a):\n",
    "\n",
    "        # Apply sigmoid all hidden layers except output layer\n",
    "        for i in xrange(self.layers-2):\n",
    "            a = sigmoid(np.dot(a, self.weights[i]) + self.biases[i])\n",
    "\n",
    "        # Apply softmax for the output layer\n",
    "        ez = np.exp(np.dot(a, self.weights[-1]) + self.biases[-1])\n",
    "        a = ez/ez.sum(axis=1, keepdims=True)\n",
    "        return a \n",
    "\n",
    "    def one_hot_encoding(self, y):\n",
    "        sz = self.sizes[-1] # Output layer size\n",
    "        oneHotY = np.zeros((len(y), sz))\n",
    "        for i in range(len(y)):\n",
    "            oneHotY[i, y[i]] = 1\n",
    "        return oneHotY\n",
    "\n",
    "    def back_propagation(self, a, y):\n",
    "\n",
    "        # Store intermediate activations and z\n",
    "        zs = []\n",
    "        activations = [a]\n",
    "        for i in range(self.layers-2):\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = sigmoid(z)\n",
    "            zs.append(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Compute output layer output (softmax)\n",
    "        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        ez = np.exp(z)\n",
    "        yHat = ez/ez.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # Compute delta at output layer (dJ/dz) - cross entory cost function\n",
    "        delta = yHat - self.one_hot_encoding(y)\n",
    "\n",
    "        # gradients (basis, weights) for each layer\n",
    "        gw = [np.zeros(w.shape) for w in self.weights]\n",
    "        gb = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        # Last layers weight and basis gradient\n",
    "        gb[-1] = delta.sum(axis=0)\n",
    "        gw[-1] = np.dot(activations[-1].T, delta)\n",
    "\n",
    "        # Remaining layers gradients (bias, weight)\n",
    "        for l in range(2, self.layers):\n",
    "            delta = np.multiply(np.dot(delta, self.weights[-l+1].T), sigmoid_prime(zs[-l+1]))\n",
    "            gb[-l] = delta.sum(axis=0)\n",
    "            gw[-l] = np.dot(activations[-l].T, delta)\n",
    "\n",
    "        return(gw, gb, yHat)\n",
    "\n",
    "    def cost(self, t, y):\n",
    "        c = t*np.log(y)\n",
    "        return (-1.0)*np.sum(c)\n",
    "\n",
    "    def gradient_descent(self, epochs, eta, x, y):\n",
    "\n",
    "        # Update weights in each iteration\n",
    "        yOneHot = N.one_hot_encoding(y)\n",
    "        for i in range(epochs):\n",
    "            gw, gb, yHat  = self.back_propagation(x, y)\n",
    "            cl_rate = classification_rate(y, np.argmax(yHat, axis=1))\n",
    "            cost = self.cost(yOneHot, yHat)\n",
    "            print (\"Iteration: \", i, \" Classification rate: \", cl_rate,  \" Cost: \", cost)\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights = [ w-((eta)/len(y))*dw for w, dw in zip(self.weights, gw)]\n",
    "            self.biases = [ b-((eta)/len(y))*db for b, db in zip(self.biases, gb)]\n",
    "\n",
    "\n",
    "# Example - Create 3 different types of clsuters with 500 points each\n",
    "npoints = 500\n",
    "X1 = np.random.randn(npoints, 2) + np.array([0, -2]) # Just add some offset\n",
    "X2 = np.random.randn(npoints, 2) + np.array([2, 2]) # Just add some offset\n",
    "X3 = np.random.randn(npoints, 2) + np.array([-2, 2]) # Just add some offset\n",
    "X = np.vstack([X1, X2, X3])\n",
    "print(X.shape)\n",
    "print(X[0])\n",
    "# Create labels for each class (0, 1 and 2)\n",
    "Y = np.array([0]*npoints + [1]*npoints + [2]*npoints)\n",
    "print(Y.shape)\n",
    "print(Y[0])\n",
    "# Create neural network and run gradient descent\n",
    "N = Network([2, 15, 3])\n",
    "#N.gradient_descent(1000, 0.001, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "X -= np.mean(X, axis=0)\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = Network([64, 32, 32, 16, 16, 16, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0  Classification rate:  0.10072342793544797  Cost:  6363.1177328627255\n",
      "Iteration:  1  Classification rate:  0.10072342793544797  Cost:  6360.835489869945\n",
      "Iteration:  2  Classification rate:  0.10127991096271564  Cost:  6358.556999822373\n",
      "Iteration:  3  Classification rate:  0.1018363939899833  Cost:  6356.282252308101\n",
      "Iteration:  4  Classification rate:  0.1018363939899833  Cost:  6354.011236954488\n",
      "Iteration:  5  Classification rate:  0.10072342793544797  Cost:  6351.743943427987\n",
      "Iteration:  6  Classification rate:  0.10072342793544797  Cost:  6349.480361433972\n",
      "Iteration:  7  Classification rate:  0.10072342793544797  Cost:  6347.220480716563\n",
      "Iteration:  8  Classification rate:  0.10072342793544797  Cost:  6344.964291058459\n",
      "Iteration:  9  Classification rate:  0.10072342793544797  Cost:  6342.711782280769\n",
      "Iteration:  10  Classification rate:  0.10072342793544797  Cost:  6340.462944242834\n",
      "Iteration:  11  Classification rate:  0.1001669449081803  Cost:  6338.217766842071\n",
      "Iteration:  12  Classification rate:  0.1001669449081803  Cost:  6335.976240013793\n",
      "Iteration:  13  Classification rate:  0.1001669449081803  Cost:  6333.738353731053\n",
      "Iteration:  14  Classification rate:  0.1001669449081803  Cost:  6331.50409800447\n",
      "Iteration:  15  Classification rate:  0.1001669449081803  Cost:  6329.273462882065\n",
      "Iteration:  16  Classification rate:  0.1001669449081803  Cost:  6327.046438449101\n",
      "Iteration:  17  Classification rate:  0.1001669449081803  Cost:  6324.823014827915\n",
      "Iteration:  18  Classification rate:  0.1001669449081803  Cost:  6322.6031821777615\n",
      "Iteration:  19  Classification rate:  0.1001669449081803  Cost:  6320.386930694639\n",
      "Iteration:  20  Classification rate:  0.1001669449081803  Cost:  6318.174250611143\n",
      "Iteration:  21  Classification rate:  0.10127991096271564  Cost:  6315.965132196292\n",
      "Iteration:  22  Classification rate:  0.10127991096271564  Cost:  6313.75956575538\n",
      "Iteration:  23  Classification rate:  0.10239287701725097  Cost:  6311.557541629811\n",
      "Iteration:  24  Classification rate:  0.10239287701725097  Cost:  6309.359050196937\n",
      "Iteration:  25  Classification rate:  0.10239287701725097  Cost:  6307.164081869913\n",
      "Iteration:  26  Classification rate:  0.10294936004451864  Cost:  6304.97262709753\n",
      "Iteration:  27  Classification rate:  0.10294936004451864  Cost:  6302.784676364065\n",
      "Iteration:  28  Classification rate:  0.10294936004451864  Cost:  6300.600220189116\n",
      "Iteration:  29  Classification rate:  0.10294936004451864  Cost:  6298.419249127463\n",
      "Iteration:  30  Classification rate:  0.10294936004451864  Cost:  6296.241753768909\n",
      "Iteration:  31  Classification rate:  0.10350584307178631  Cost:  6294.067724738112\n",
      "Iteration:  32  Classification rate:  0.10406232609905398  Cost:  6291.897152694459\n",
      "Iteration:  33  Classification rate:  0.10406232609905398  Cost:  6289.730028331891\n",
      "Iteration:  34  Classification rate:  0.10406232609905398  Cost:  6287.566342378772\n",
      "Iteration:  35  Classification rate:  0.10406232609905398  Cost:  6285.40608559772\n",
      "Iteration:  36  Classification rate:  0.10406232609905398  Cost:  6283.249248785477\n",
      "Iteration:  37  Classification rate:  0.10406232609905398  Cost:  6281.095822772741\n",
      "Iteration:  38  Classification rate:  0.10406232609905398  Cost:  6278.945798424032\n",
      "Iteration:  39  Classification rate:  0.10406232609905398  Cost:  6276.799166637542\n",
      "Iteration:  40  Classification rate:  0.10406232609905398  Cost:  6274.655918344983\n",
      "Iteration:  41  Classification rate:  0.10406232609905398  Cost:  6272.516044511447\n",
      "Iteration:  42  Classification rate:  0.10406232609905398  Cost:  6270.379536135259\n",
      "Iteration:  43  Classification rate:  0.10406232609905398  Cost:  6268.246384247829\n",
      "Iteration:  44  Classification rate:  0.10406232609905398  Cost:  6266.116579913515\n",
      "Iteration:  45  Classification rate:  0.10461880912632164  Cost:  6263.990114229475\n",
      "Iteration:  46  Classification rate:  0.10517529215358931  Cost:  6261.866978325523\n",
      "Iteration:  47  Classification rate:  0.10573177518085698  Cost:  6259.74716336399\n",
      "Iteration:  48  Classification rate:  0.10517529215358931  Cost:  6257.630660539582\n",
      "Iteration:  49  Classification rate:  0.10517529215358931  Cost:  6255.517461079241\n",
      "Iteration:  50  Classification rate:  0.10517529215358931  Cost:  6253.407556242001\n",
      "Iteration:  51  Classification rate:  0.10517529215358931  Cost:  6251.300937318852\n",
      "Iteration:  52  Classification rate:  0.10517529215358931  Cost:  6249.1975956326\n",
      "Iteration:  53  Classification rate:  0.10517529215358931  Cost:  6247.097522537725\n",
      "Iteration:  54  Classification rate:  0.10517529215358931  Cost:  6245.000709420253\n",
      "Iteration:  55  Classification rate:  0.10517529215358931  Cost:  6242.907147697609\n",
      "Iteration:  56  Classification rate:  0.10517529215358931  Cost:  6240.816828818486\n",
      "Iteration:  57  Classification rate:  0.10573177518085698  Cost:  6238.729744262709\n",
      "Iteration:  58  Classification rate:  0.10573177518085698  Cost:  6236.645885541096\n",
      "Iteration:  59  Classification rate:  0.10573177518085698  Cost:  6234.565244195331\n",
      "Iteration:  60  Classification rate:  0.10573177518085698  Cost:  6232.4878117978205\n",
      "Iteration:  61  Classification rate:  0.10573177518085698  Cost:  6230.41357995157\n",
      "Iteration:  62  Classification rate:  0.10573177518085698  Cost:  6228.342540290039\n",
      "Iteration:  63  Classification rate:  0.10573177518085698  Cost:  6226.2746844770245\n",
      "Iteration:  64  Classification rate:  0.10573177518085698  Cost:  6224.210004206514\n",
      "Iteration:  65  Classification rate:  0.10573177518085698  Cost:  6222.148491202567\n",
      "Iteration:  66  Classification rate:  0.10573177518085698  Cost:  6220.090137219174\n",
      "Iteration:  67  Classification rate:  0.10573177518085698  Cost:  6218.034934040133\n",
      "Iteration:  68  Classification rate:  0.10573177518085698  Cost:  6215.982873478919\n",
      "Iteration:  69  Classification rate:  0.10573177518085698  Cost:  6213.933947378558\n",
      "Iteration:  70  Classification rate:  0.10573177518085698  Cost:  6211.8881476114875\n",
      "Iteration:  71  Classification rate:  0.10573177518085698  Cost:  6209.845466079446\n",
      "Iteration:  72  Classification rate:  0.10573177518085698  Cost:  6207.80589471333\n",
      "Iteration:  73  Classification rate:  0.10573177518085698  Cost:  6205.7694254730795\n",
      "Iteration:  74  Classification rate:  0.10573177518085698  Cost:  6203.736050347542\n",
      "Iteration:  75  Classification rate:  0.10573177518085698  Cost:  6201.705761354354\n",
      "Iteration:  76  Classification rate:  0.10573177518085698  Cost:  6199.678550539816\n",
      "Iteration:  77  Classification rate:  0.10573177518085698  Cost:  6197.6544099787625\n",
      "Iteration:  78  Classification rate:  0.10573177518085698  Cost:  6195.633331774442\n",
      "Iteration:  79  Classification rate:  0.10517529215358931  Cost:  6193.615308058395\n",
      "Iteration:  80  Classification rate:  0.10461880912632164  Cost:  6191.600330990331\n",
      "Iteration:  81  Classification rate:  0.10461880912632164  Cost:  6189.588392758\n",
      "Iteration:  82  Classification rate:  0.10461880912632164  Cost:  6187.579485577078\n",
      "Iteration:  83  Classification rate:  0.10461880912632164  Cost:  6185.573601691045\n",
      "Iteration:  84  Classification rate:  0.10461880912632164  Cost:  6183.57073337106\n",
      "Iteration:  85  Classification rate:  0.10461880912632164  Cost:  6181.570872915845\n",
      "Iteration:  86  Classification rate:  0.10517529215358931  Cost:  6179.574012651563\n",
      "Iteration:  87  Classification rate:  0.10517529215358931  Cost:  6177.5801449317005\n",
      "Iteration:  88  Classification rate:  0.10517529215358931  Cost:  6175.589262136948\n",
      "Iteration:  89  Classification rate:  0.10517529215358931  Cost:  6173.601356675082\n",
      "Iteration:  90  Classification rate:  0.10573177518085698  Cost:  6171.616420980843\n",
      "Iteration:  91  Classification rate:  0.10573177518085698  Cost:  6169.634447515832\n",
      "Iteration:  92  Classification rate:  0.10573177518085698  Cost:  6167.655428768374\n",
      "Iteration:  93  Classification rate:  0.10795770728992765  Cost:  6165.679357253421\n",
      "Iteration:  94  Classification rate:  0.10795770728992765  Cost:  6163.706225512422\n",
      "Iteration:  95  Classification rate:  0.10851419031719532  Cost:  6161.736026113218\n",
      "Iteration:  96  Classification rate:  0.10851419031719532  Cost:  6159.768751649919\n",
      "Iteration:  97  Classification rate:  0.10851419031719532  Cost:  6157.804394742798\n",
      "Iteration:  98  Classification rate:  0.10851419031719532  Cost:  6155.842948038169\n",
      "Iteration:  99  Classification rate:  0.10851419031719532  Cost:  6153.884404208283\n",
      "Iteration:  100  Classification rate:  0.10851419031719532  Cost:  6151.928755951205\n",
      "Iteration:  101  Classification rate:  0.10851419031719532  Cost:  6149.975995990711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  102  Classification rate:  0.10851419031719532  Cost:  6148.026117076171\n",
      "Iteration:  103  Classification rate:  0.10851419031719532  Cost:  6146.079111982441\n",
      "Iteration:  104  Classification rate:  0.10851419031719532  Cost:  6144.134973509747\n",
      "Iteration:  105  Classification rate:  0.10851419031719532  Cost:  6142.193694483581\n",
      "Iteration:  106  Classification rate:  0.10851419031719532  Cost:  6140.255267754587\n",
      "Iteration:  107  Classification rate:  0.10851419031719532  Cost:  6138.319686198454\n",
      "Iteration:  108  Classification rate:  0.10851419031719532  Cost:  6136.386942715805\n",
      "Iteration:  109  Classification rate:  0.10907067334446299  Cost:  6134.457030232091\n",
      "Iteration:  110  Classification rate:  0.10907067334446299  Cost:  6132.529941697478\n",
      "Iteration:  111  Classification rate:  0.10907067334446299  Cost:  6130.605670086748\n",
      "Iteration:  112  Classification rate:  0.10962715637173066  Cost:  6128.684208399184\n",
      "Iteration:  113  Classification rate:  0.11018363939899833  Cost:  6126.7655496584675\n",
      "Iteration:  114  Classification rate:  0.110740122426266  Cost:  6124.849686912572\n",
      "Iteration:  115  Classification rate:  0.110740122426266  Cost:  6122.9366132336545\n",
      "Iteration:  116  Classification rate:  0.110740122426266  Cost:  6121.026321717954\n",
      "Iteration:  117  Classification rate:  0.110740122426266  Cost:  6119.118805485686\n",
      "Iteration:  118  Classification rate:  0.110740122426266  Cost:  6117.214057680935\n",
      "Iteration:  119  Classification rate:  0.110740122426266  Cost:  6115.312071471557\n",
      "Iteration:  120  Classification rate:  0.110740122426266  Cost:  6113.412840049067\n",
      "Iteration:  121  Classification rate:  0.110740122426266  Cost:  6111.516356628544\n",
      "Iteration:  122  Classification rate:  0.11129660545353366  Cost:  6109.622614448526\n",
      "Iteration:  123  Classification rate:  0.11129660545353366  Cost:  6107.731606770906\n",
      "Iteration:  124  Classification rate:  0.11185308848080133  Cost:  6105.843326880832\n",
      "Iteration:  125  Classification rate:  0.11185308848080133  Cost:  6103.957768086607\n",
      "Iteration:  126  Classification rate:  0.11296605453533667  Cost:  6102.074923719583\n",
      "Iteration:  127  Classification rate:  0.112409571508069  Cost:  6100.194787134071\n",
      "Iteration:  128  Classification rate:  0.112409571508069  Cost:  6098.3173517072255\n",
      "Iteration:  129  Classification rate:  0.112409571508069  Cost:  6096.4426108389625\n",
      "Iteration:  130  Classification rate:  0.112409571508069  Cost:  6094.570557951845\n",
      "Iteration:  131  Classification rate:  0.112409571508069  Cost:  6092.701186490999\n",
      "Iteration:  132  Classification rate:  0.112409571508069  Cost:  6090.834489924\n",
      "Iteration:  133  Classification rate:  0.112409571508069  Cost:  6088.970461740784\n",
      "Iteration:  134  Classification rate:  0.112409571508069  Cost:  6087.109095453554\n",
      "Iteration:  135  Classification rate:  0.112409571508069  Cost:  6085.250384596673\n",
      "Iteration:  136  Classification rate:  0.112409571508069  Cost:  6083.394322726575\n",
      "Iteration:  137  Classification rate:  0.11352253756260434  Cost:  6081.540903421664\n",
      "Iteration:  138  Classification rate:  0.11352253756260434  Cost:  6079.690120282222\n",
      "Iteration:  139  Classification rate:  0.11352253756260434  Cost:  6077.841966930313\n",
      "Iteration:  140  Classification rate:  0.11352253756260434  Cost:  6075.996437009688\n",
      "Iteration:  141  Classification rate:  0.11407902058987202  Cost:  6074.153524185687\n",
      "Iteration:  142  Classification rate:  0.11407902058987202  Cost:  6072.313222145151\n",
      "Iteration:  143  Classification rate:  0.11463550361713967  Cost:  6070.475524596323\n",
      "Iteration:  144  Classification rate:  0.11463550361713967  Cost:  6068.64042526876\n",
      "Iteration:  145  Classification rate:  0.11463550361713967  Cost:  6066.807917913235\n",
      "Iteration:  146  Classification rate:  0.11463550361713967  Cost:  6064.977996301649\n",
      "Iteration:  147  Classification rate:  0.11519198664440734  Cost:  6063.150654226934\n",
      "Iteration:  148  Classification rate:  0.11519198664440734  Cost:  6061.325885502969\n",
      "Iteration:  149  Classification rate:  0.11574846967167501  Cost:  6059.50368396448\n",
      "Iteration:  150  Classification rate:  0.11519198664440734  Cost:  6057.684043466955\n",
      "Iteration:  151  Classification rate:  0.11519198664440734  Cost:  6055.866957886552\n",
      "Iteration:  152  Classification rate:  0.11630495269894268  Cost:  6054.052421120008\n",
      "Iteration:  153  Classification rate:  0.11686143572621036  Cost:  6052.240427084555\n",
      "Iteration:  154  Classification rate:  0.11686143572621036  Cost:  6050.43096971782\n",
      "Iteration:  155  Classification rate:  0.11686143572621036  Cost:  6048.6240429777445\n",
      "Iteration:  156  Classification rate:  0.11686143572621036  Cost:  6046.819640842495\n",
      "Iteration:  157  Classification rate:  0.11686143572621036  Cost:  6045.017757310375\n",
      "Iteration:  158  Classification rate:  0.11686143572621036  Cost:  6043.218386399733\n",
      "Iteration:  159  Classification rate:  0.11686143572621036  Cost:  6041.421522148883\n",
      "Iteration:  160  Classification rate:  0.11741791875347801  Cost:  6039.627158616008\n",
      "Iteration:  161  Classification rate:  0.11741791875347801  Cost:  6037.835289879084\n",
      "Iteration:  162  Classification rate:  0.11741791875347801  Cost:  6036.045910035784\n",
      "Iteration:  163  Classification rate:  0.11741791875347801  Cost:  6034.259013203402\n",
      "Iteration:  164  Classification rate:  0.11741791875347801  Cost:  6032.474593518757\n",
      "Iteration:  165  Classification rate:  0.11741791875347801  Cost:  6030.692645138115\n",
      "Iteration:  166  Classification rate:  0.11741791875347801  Cost:  6028.913162237108\n",
      "Iteration:  167  Classification rate:  0.11741791875347801  Cost:  6027.136139010639\n",
      "Iteration:  168  Classification rate:  0.11686143572621036  Cost:  6025.361569672808\n",
      "Iteration:  169  Classification rate:  0.11686143572621036  Cost:  6023.5894484568225\n",
      "Iteration:  170  Classification rate:  0.11686143572621036  Cost:  6021.819769614916\n",
      "Iteration:  171  Classification rate:  0.11686143572621036  Cost:  6020.052527418268\n",
      "Iteration:  172  Classification rate:  0.11686143572621036  Cost:  6018.28771615692\n",
      "Iteration:  173  Classification rate:  0.11686143572621036  Cost:  6016.525330139689\n",
      "Iteration:  174  Classification rate:  0.11686143572621036  Cost:  6014.765363694094\n",
      "Iteration:  175  Classification rate:  0.11686143572621036  Cost:  6013.007811166269\n",
      "Iteration:  176  Classification rate:  0.11741791875347801  Cost:  6011.2526669208855\n",
      "Iteration:  177  Classification rate:  0.11797440178074568  Cost:  6009.499925341068\n",
      "Iteration:  178  Classification rate:  0.11797440178074568  Cost:  6007.74958082832\n",
      "Iteration:  179  Classification rate:  0.11797440178074568  Cost:  6006.001627802438\n",
      "Iteration:  180  Classification rate:  0.11797440178074568  Cost:  6004.256060701432\n",
      "Iteration:  181  Classification rate:  0.11797440178074568  Cost:  6002.512873981455\n",
      "Iteration:  182  Classification rate:  0.11797440178074568  Cost:  6000.772062116717\n",
      "Iteration:  183  Classification rate:  0.11741791875347801  Cost:  5999.033619599404\n",
      "Iteration:  184  Classification rate:  0.11797440178074568  Cost:  5997.2975409396095\n",
      "Iteration:  185  Classification rate:  0.11797440178074568  Cost:  5995.563820665249\n",
      "Iteration:  186  Classification rate:  0.11797440178074568  Cost:  5993.832453321985\n",
      "Iteration:  187  Classification rate:  0.11797440178074568  Cost:  5992.103433473151\n",
      "Iteration:  188  Classification rate:  0.11853088480801335  Cost:  5990.376755699675\n",
      "Iteration:  189  Classification rate:  0.11853088480801335  Cost:  5988.652414600003\n",
      "Iteration:  190  Classification rate:  0.11853088480801335  Cost:  5986.930404790021\n",
      "Iteration:  191  Classification rate:  0.11853088480801335  Cost:  5985.210720902984\n",
      "Iteration:  192  Classification rate:  0.11853088480801335  Cost:  5983.493357589436\n",
      "Iteration:  193  Classification rate:  0.11853088480801335  Cost:  5981.778309517136\n",
      "Iteration:  194  Classification rate:  0.11853088480801335  Cost:  5980.065571370991\n",
      "Iteration:  195  Classification rate:  0.11853088480801335  Cost:  5978.35513785297\n",
      "Iteration:  196  Classification rate:  0.11853088480801335  Cost:  5976.647003682041\n",
      "Iteration:  197  Classification rate:  0.11908736783528102  Cost:  5974.941163594084\n",
      "Iteration:  198  Classification rate:  0.11908736783528102  Cost:  5973.237612341837\n",
      "Iteration:  199  Classification rate:  0.11908736783528102  Cost:  5971.536344694809\n",
      "Iteration:  200  Classification rate:  0.1196438508625487  Cost:  5969.837355439205\n",
      "Iteration:  201  Classification rate:  0.11908736783528102  Cost:  5968.140639377872\n",
      "Iteration:  202  Classification rate:  0.11908736783528102  Cost:  5966.446191330206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  203  Classification rate:  0.11908736783528102  Cost:  5964.754006132093\n",
      "Iteration:  204  Classification rate:  0.1196438508625487  Cost:  5963.064078635838\n",
      "Iteration:  205  Classification rate:  0.12020033388981637  Cost:  5961.376403710086\n",
      "Iteration:  206  Classification rate:  0.12020033388981637  Cost:  5959.6909762397645\n",
      "Iteration:  207  Classification rate:  0.12020033388981637  Cost:  5958.007791125997\n",
      "Iteration:  208  Classification rate:  0.1196438508625487  Cost:  5956.326843286047\n",
      "Iteration:  209  Classification rate:  0.1196438508625487  Cost:  5954.648127653244\n",
      "Iteration:  210  Classification rate:  0.12020033388981637  Cost:  5952.9716391769125\n",
      "Iteration:  211  Classification rate:  0.12020033388981637  Cost:  5951.297372822304\n",
      "Iteration:  212  Classification rate:  0.12020033388981637  Cost:  5949.625323570533\n",
      "Iteration:  213  Classification rate:  0.12020033388981637  Cost:  5947.9554864185\n",
      "Iteration:  214  Classification rate:  0.12020033388981637  Cost:  5946.287856378833\n",
      "Iteration:  215  Classification rate:  0.12020033388981637  Cost:  5944.622428479814\n",
      "Iteration:  216  Classification rate:  0.12020033388981637  Cost:  5942.959197765314\n",
      "Iteration:  217  Classification rate:  0.12075681691708402  Cost:  5941.298159294724\n",
      "Iteration:  218  Classification rate:  0.12075681691708402  Cost:  5939.639308142896\n",
      "Iteration:  219  Classification rate:  0.12075681691708402  Cost:  5937.982639400067\n",
      "Iteration:  220  Classification rate:  0.12075681691708402  Cost:  5936.328148171792\n",
      "Iteration:  221  Classification rate:  0.12075681691708402  Cost:  5934.675829578894\n",
      "Iteration:  222  Classification rate:  0.12075681691708402  Cost:  5933.025678757382\n",
      "Iteration:  223  Classification rate:  0.12075681691708402  Cost:  5931.377690858393\n",
      "Iteration:  224  Classification rate:  0.12075681691708402  Cost:  5929.731861048129\n",
      "Iteration:  225  Classification rate:  0.12075681691708402  Cost:  5928.0881845077865\n",
      "Iteration:  226  Classification rate:  0.1196438508625487  Cost:  5926.446656433499\n",
      "Iteration:  227  Classification rate:  0.12020033388981637  Cost:  5924.807272036271\n",
      "Iteration:  228  Classification rate:  0.12075681691708402  Cost:  5923.1700265419095\n",
      "Iteration:  229  Classification rate:  0.12075681691708402  Cost:  5921.534915190974\n",
      "Iteration:  230  Classification rate:  0.12075681691708402  Cost:  5919.901933238697\n",
      "Iteration:  231  Classification rate:  0.12075681691708402  Cost:  5918.271075954932\n",
      "Iteration:  232  Classification rate:  0.12075681691708402  Cost:  5916.64233862409\n",
      "Iteration:  233  Classification rate:  0.1213132999443517  Cost:  5915.015716545078\n",
      "Iteration:  234  Classification rate:  0.12075681691708402  Cost:  5913.391205031232\n",
      "Iteration:  235  Classification rate:  0.12075681691708402  Cost:  5911.76879941026\n",
      "Iteration:  236  Classification rate:  0.12075681691708402  Cost:  5910.148495024184\n",
      "Iteration:  237  Classification rate:  0.12075681691708402  Cost:  5908.530287229273\n",
      "Iteration:  238  Classification rate:  0.12075681691708402  Cost:  5906.914171395986\n",
      "Iteration:  239  Classification rate:  0.12075681691708402  Cost:  5905.300142908911\n",
      "Iteration:  240  Classification rate:  0.12020033388981637  Cost:  5903.6881971667035\n",
      "Iteration:  241  Classification rate:  0.12020033388981637  Cost:  5902.078329582033\n",
      "Iteration:  242  Classification rate:  0.1196438508625487  Cost:  5900.470535581516\n",
      "Iteration:  243  Classification rate:  0.1196438508625487  Cost:  5898.864810605662\n",
      "Iteration:  244  Classification rate:  0.1196438508625487  Cost:  5897.26115010881\n",
      "Iteration:  245  Classification rate:  0.1196438508625487  Cost:  5895.659549559079\n",
      "Iteration:  246  Classification rate:  0.1196438508625487  Cost:  5894.060004438302\n",
      "Iteration:  247  Classification rate:  0.1196438508625487  Cost:  5892.462510241965\n",
      "Iteration:  248  Classification rate:  0.12020033388981637  Cost:  5890.867062479163\n",
      "Iteration:  249  Classification rate:  0.12020033388981637  Cost:  5889.27365667253\n",
      "Iteration:  250  Classification rate:  0.12020033388981637  Cost:  5887.682288358188\n",
      "Iteration:  251  Classification rate:  0.12020033388981637  Cost:  5886.092953085687\n",
      "Iteration:  252  Classification rate:  0.12020033388981637  Cost:  5884.505646417953\n",
      "Iteration:  253  Classification rate:  0.12020033388981637  Cost:  5882.920363931226\n",
      "Iteration:  254  Classification rate:  0.12020033388981637  Cost:  5881.337101215011\n",
      "Iteration:  255  Classification rate:  0.12020033388981637  Cost:  5879.755853872015\n",
      "Iteration:  256  Classification rate:  0.12020033388981637  Cost:  5878.176617518098\n",
      "Iteration:  257  Classification rate:  0.12020033388981637  Cost:  5876.599387782214\n",
      "Iteration:  258  Classification rate:  0.12020033388981637  Cost:  5875.024160306357\n",
      "Iteration:  259  Classification rate:  0.12020033388981637  Cost:  5873.45093074551\n",
      "Iteration:  260  Classification rate:  0.1196438508625487  Cost:  5871.879694767584\n",
      "Iteration:  261  Classification rate:  0.11908736783528102  Cost:  5870.310448053369\n",
      "Iteration:  262  Classification rate:  0.11908736783528102  Cost:  5868.743186296479\n",
      "Iteration:  263  Classification rate:  0.11853088480801335  Cost:  5867.177905203298\n",
      "Iteration:  264  Classification rate:  0.11853088480801335  Cost:  5865.6146004929315\n",
      "Iteration:  265  Classification rate:  0.11853088480801335  Cost:  5864.053267897142\n",
      "Iteration:  266  Classification rate:  0.11797440178074568  Cost:  5862.49390316031\n",
      "Iteration:  267  Classification rate:  0.11797440178074568  Cost:  5860.936502039371\n",
      "Iteration:  268  Classification rate:  0.11797440178074568  Cost:  5859.381060303772\n",
      "Iteration:  269  Classification rate:  0.11797440178074568  Cost:  5857.8275737354115\n",
      "Iteration:  270  Classification rate:  0.11797440178074568  Cost:  5856.276038128595\n",
      "Iteration:  271  Classification rate:  0.11797440178074568  Cost:  5854.726449289978\n",
      "Iteration:  272  Classification rate:  0.11797440178074568  Cost:  5853.178803038516\n",
      "Iteration:  273  Classification rate:  0.11797440178074568  Cost:  5851.6330952054195\n",
      "Iteration:  274  Classification rate:  0.11853088480801335  Cost:  5850.089321634095\n",
      "Iteration:  275  Classification rate:  0.11853088480801335  Cost:  5848.5474781800995\n",
      "Iteration:  276  Classification rate:  0.11853088480801335  Cost:  5847.00756071109\n",
      "Iteration:  277  Classification rate:  0.11908736783528102  Cost:  5845.469565106773\n",
      "Iteration:  278  Classification rate:  0.11908736783528102  Cost:  5843.933487258853\n",
      "Iteration:  279  Classification rate:  0.11908736783528102  Cost:  5842.3993230709875\n",
      "Iteration:  280  Classification rate:  0.11853088480801335  Cost:  5840.867068458734\n",
      "Iteration:  281  Classification rate:  0.11853088480801335  Cost:  5839.336719349504\n",
      "Iteration:  282  Classification rate:  0.11908736783528102  Cost:  5837.808271682512\n",
      "Iteration:  283  Classification rate:  0.11908736783528102  Cost:  5836.281721408727\n",
      "Iteration:  284  Classification rate:  0.11908736783528102  Cost:  5834.757064490831\n",
      "Iteration:  285  Classification rate:  0.11908736783528102  Cost:  5833.234296903158\n",
      "Iteration:  286  Classification rate:  0.11908736783528102  Cost:  5831.71341463166\n",
      "Iteration:  287  Classification rate:  0.11908736783528102  Cost:  5830.194413673851\n",
      "Iteration:  288  Classification rate:  0.11908736783528102  Cost:  5828.677290038762\n",
      "Iteration:  289  Classification rate:  0.11853088480801335  Cost:  5827.162039746897\n",
      "Iteration:  290  Classification rate:  0.11853088480801335  Cost:  5825.648658830184\n",
      "Iteration:  291  Classification rate:  0.11853088480801335  Cost:  5824.1371433319255\n",
      "Iteration:  292  Classification rate:  0.11853088480801335  Cost:  5822.6274893067575\n",
      "Iteration:  293  Classification rate:  0.11853088480801335  Cost:  5821.119692820603\n",
      "Iteration:  294  Classification rate:  0.11853088480801335  Cost:  5819.6137499506185\n",
      "Iteration:  295  Classification rate:  0.11908736783528102  Cost:  5818.109656785165\n",
      "Iteration:  296  Classification rate:  0.11908736783528102  Cost:  5816.607409423741\n",
      "Iteration:  297  Classification rate:  0.1196438508625487  Cost:  5815.107003976956\n",
      "Iteration:  298  Classification rate:  0.12020033388981637  Cost:  5813.608436566476\n",
      "Iteration:  299  Classification rate:  0.12020033388981637  Cost:  5812.111703324985\n",
      "Iteration:  300  Classification rate:  0.12020033388981637  Cost:  5810.61680039613\n",
      "Iteration:  301  Classification rate:  0.12020033388981637  Cost:  5809.12372393449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  302  Classification rate:  0.12020033388981637  Cost:  5807.6324701055255\n",
      "Iteration:  303  Classification rate:  0.12020033388981637  Cost:  5806.143035085532\n",
      "Iteration:  304  Classification rate:  0.12020033388981637  Cost:  5804.655415061603\n",
      "Iteration:  305  Classification rate:  0.12020033388981637  Cost:  5803.169606231581\n",
      "Iteration:  306  Classification rate:  0.12020033388981637  Cost:  5801.685604804023\n",
      "Iteration:  307  Classification rate:  0.12020033388981637  Cost:  5800.203406998142\n",
      "Iteration:  308  Classification rate:  0.12020033388981637  Cost:  5798.723009043782\n",
      "Iteration:  309  Classification rate:  0.12020033388981637  Cost:  5797.244407181368\n",
      "Iteration:  310  Classification rate:  0.12020033388981637  Cost:  5795.767597661858\n",
      "Iteration:  311  Classification rate:  0.1196438508625487  Cost:  5794.292576746714\n",
      "Iteration:  312  Classification rate:  0.11908736783528102  Cost:  5792.819340707848\n",
      "Iteration:  313  Classification rate:  0.11908736783528102  Cost:  5791.347885827587\n",
      "Iteration:  314  Classification rate:  0.11908736783528102  Cost:  5789.878208398632\n",
      "Iteration:  315  Classification rate:  0.11908736783528102  Cost:  5788.4103047240105\n",
      "Iteration:  316  Classification rate:  0.11908736783528102  Cost:  5786.9441711170475\n",
      "Iteration:  317  Classification rate:  0.11908736783528102  Cost:  5785.479803901311\n",
      "Iteration:  318  Classification rate:  0.11908736783528102  Cost:  5784.017199410585\n",
      "Iteration:  319  Classification rate:  0.11908736783528102  Cost:  5782.556353988815\n",
      "Iteration:  320  Classification rate:  0.11908736783528102  Cost:  5781.097263990081\n",
      "Iteration:  321  Classification rate:  0.11908736783528102  Cost:  5779.6399257785515\n",
      "Iteration:  322  Classification rate:  0.1196438508625487  Cost:  5778.184335728445\n",
      "Iteration:  323  Classification rate:  0.1196438508625487  Cost:  5776.730490223984\n",
      "Iteration:  324  Classification rate:  0.1196438508625487  Cost:  5775.278385659371\n",
      "Iteration:  325  Classification rate:  0.12020033388981637  Cost:  5773.828018438737\n",
      "Iteration:  326  Classification rate:  0.12075681691708402  Cost:  5772.379384976103\n",
      "Iteration:  327  Classification rate:  0.12020033388981637  Cost:  5770.932481695348\n",
      "Iteration:  328  Classification rate:  0.12020033388981637  Cost:  5769.487305030166\n",
      "Iteration:  329  Classification rate:  0.12020033388981637  Cost:  5768.043851424029\n",
      "Iteration:  330  Classification rate:  0.12020033388981637  Cost:  5766.60211733015\n",
      "Iteration:  331  Classification rate:  0.1196438508625487  Cost:  5765.16209921144\n",
      "Iteration:  332  Classification rate:  0.1196438508625487  Cost:  5763.723793540484\n",
      "Iteration:  333  Classification rate:  0.1196438508625487  Cost:  5762.28719679948\n",
      "Iteration:  334  Classification rate:  0.1196438508625487  Cost:  5760.852305480226\n",
      "Iteration:  335  Classification rate:  0.1196438508625487  Cost:  5759.419116084071\n",
      "Iteration:  336  Classification rate:  0.12020033388981637  Cost:  5757.987625121878\n",
      "Iteration:  337  Classification rate:  0.12020033388981637  Cost:  5756.55782911399\n",
      "Iteration:  338  Classification rate:  0.12020033388981637  Cost:  5755.129724590193\n",
      "Iteration:  339  Classification rate:  0.12020033388981637  Cost:  5753.703308089678\n",
      "Iteration:  340  Classification rate:  0.1196438508625487  Cost:  5752.27857616101\n",
      "Iteration:  341  Classification rate:  0.1196438508625487  Cost:  5750.855525362083\n",
      "Iteration:  342  Classification rate:  0.1196438508625487  Cost:  5749.434152260094\n",
      "Iteration:  343  Classification rate:  0.12020033388981637  Cost:  5748.014453431503\n",
      "Iteration:  344  Classification rate:  0.12020033388981637  Cost:  5746.596425461995\n",
      "Iteration:  345  Classification rate:  0.12075681691708402  Cost:  5745.180064946448\n",
      "Iteration:  346  Classification rate:  0.12075681691708402  Cost:  5743.7653684889\n",
      "Iteration:  347  Classification rate:  0.12075681691708402  Cost:  5742.35233270251\n",
      "Iteration:  348  Classification rate:  0.12075681691708402  Cost:  5740.940954209527\n",
      "Iteration:  349  Classification rate:  0.1213132999443517  Cost:  5739.531229641251\n",
      "Iteration:  350  Classification rate:  0.1213132999443517  Cost:  5738.1231556380035\n",
      "Iteration:  351  Classification rate:  0.1213132999443517  Cost:  5736.716728849091\n",
      "Iteration:  352  Classification rate:  0.1213132999443517  Cost:  5735.311945932768\n",
      "Iteration:  353  Classification rate:  0.1213132999443517  Cost:  5733.908803556215\n",
      "Iteration:  354  Classification rate:  0.1213132999443517  Cost:  5732.507298395489\n",
      "Iteration:  355  Classification rate:  0.1213132999443517  Cost:  5731.107427135497\n",
      "Iteration:  356  Classification rate:  0.1213132999443517  Cost:  5729.709186469968\n",
      "Iteration:  357  Classification rate:  0.1213132999443517  Cost:  5728.312573101417\n",
      "Iteration:  358  Classification rate:  0.1213132999443517  Cost:  5726.917583741105\n",
      "Iteration:  359  Classification rate:  0.1213132999443517  Cost:  5725.524215109013\n",
      "Iteration:  360  Classification rate:  0.1213132999443517  Cost:  5724.132463933812\n",
      "Iteration:  361  Classification rate:  0.1213132999443517  Cost:  5722.742326952823\n",
      "Iteration:  362  Classification rate:  0.1213132999443517  Cost:  5721.353800911989\n",
      "Iteration:  363  Classification rate:  0.1213132999443517  Cost:  5719.966882565845\n",
      "Iteration:  364  Classification rate:  0.1213132999443517  Cost:  5718.581568677483\n",
      "Iteration:  365  Classification rate:  0.1213132999443517  Cost:  5717.197856018519\n",
      "Iteration:  366  Classification rate:  0.1213132999443517  Cost:  5715.815741369064\n",
      "Iteration:  367  Classification rate:  0.1213132999443517  Cost:  5714.435221517697\n",
      "Iteration:  368  Classification rate:  0.1213132999443517  Cost:  5713.056293261423\n",
      "Iteration:  369  Classification rate:  0.12186978297161936  Cost:  5711.678953405649\n",
      "Iteration:  370  Classification rate:  0.12186978297161936  Cost:  5710.303198764153\n",
      "Iteration:  371  Classification rate:  0.12186978297161936  Cost:  5708.929026159052\n",
      "Iteration:  372  Classification rate:  0.12242626599888703  Cost:  5707.5564324207735\n",
      "Iteration:  373  Classification rate:  0.12242626599888703  Cost:  5706.185414388019\n",
      "Iteration:  374  Classification rate:  0.1229827490261547  Cost:  5704.815968907741\n",
      "Iteration:  375  Classification rate:  0.1229827490261547  Cost:  5703.448092835109\n",
      "Iteration:  376  Classification rate:  0.1229827490261547  Cost:  5702.081783033482\n",
      "Iteration:  377  Classification rate:  0.1229827490261547  Cost:  5700.717036374373\n",
      "Iteration:  378  Classification rate:  0.1229827490261547  Cost:  5699.353849737428\n",
      "Iteration:  379  Classification rate:  0.1229827490261547  Cost:  5697.992220010388\n",
      "Iteration:  380  Classification rate:  0.1229827490261547  Cost:  5696.632144089069\n",
      "Iteration:  381  Classification rate:  0.1229827490261547  Cost:  5695.273618877319\n",
      "Iteration:  382  Classification rate:  0.12242626599888703  Cost:  5693.916641287006\n",
      "Iteration:  383  Classification rate:  0.12242626599888703  Cost:  5692.561208237976\n",
      "Iteration:  384  Classification rate:  0.12242626599888703  Cost:  5691.207316658029\n",
      "Iteration:  385  Classification rate:  0.12242626599888703  Cost:  5689.854963482892\n",
      "Iteration:  386  Classification rate:  0.12242626599888703  Cost:  5688.504145656185\n",
      "Iteration:  387  Classification rate:  0.12186978297161936  Cost:  5687.154860129402\n",
      "Iteration:  388  Classification rate:  0.12186978297161936  Cost:  5685.807103861871\n",
      "Iteration:  389  Classification rate:  0.12186978297161936  Cost:  5684.460873820737\n",
      "Iteration:  390  Classification rate:  0.12186978297161936  Cost:  5683.116166980928\n",
      "Iteration:  391  Classification rate:  0.12186978297161936  Cost:  5681.772980325128\n",
      "Iteration:  392  Classification rate:  0.1213132999443517  Cost:  5680.431310843749\n",
      "Iteration:  393  Classification rate:  0.1213132999443517  Cost:  5679.09115553491\n",
      "Iteration:  394  Classification rate:  0.1213132999443517  Cost:  5677.752511404398\n",
      "Iteration:  395  Classification rate:  0.1213132999443517  Cost:  5676.415375465651\n",
      "Iteration:  396  Classification rate:  0.1213132999443517  Cost:  5675.079744739725\n",
      "Iteration:  397  Classification rate:  0.1213132999443517  Cost:  5673.745616255271\n",
      "Iteration:  398  Classification rate:  0.1213132999443517  Cost:  5672.412987048507\n",
      "Iteration:  399  Classification rate:  0.1213132999443517  Cost:  5671.081854163188\n",
      "Iteration:  400  Classification rate:  0.1213132999443517  Cost:  5669.752214650585\n",
      "Iteration:  401  Classification rate:  0.1213132999443517  Cost:  5668.424065569458\n",
      "Iteration:  402  Classification rate:  0.1213132999443517  Cost:  5667.097403986023\n",
      "Iteration:  403  Classification rate:  0.1213132999443517  Cost:  5665.772226973935\n",
      "Iteration:  404  Classification rate:  0.1213132999443517  Cost:  5664.448531614259\n",
      "Iteration:  405  Classification rate:  0.1213132999443517  Cost:  5663.1263149954375\n",
      "Iteration:  406  Classification rate:  0.1213132999443517  Cost:  5661.805574213276\n",
      "Iteration:  407  Classification rate:  0.12186978297161936  Cost:  5660.48630637091\n",
      "Iteration:  408  Classification rate:  0.12186978297161936  Cost:  5659.168508578782\n",
      "Iteration:  409  Classification rate:  0.12186978297161936  Cost:  5657.852177954615\n",
      "Iteration:  410  Classification rate:  0.1213132999443517  Cost:  5656.537311623391\n",
      "Iteration:  411  Classification rate:  0.1213132999443517  Cost:  5655.22390671732\n",
      "Iteration:  412  Classification rate:  0.1213132999443517  Cost:  5653.911960375819\n",
      "Iteration:  413  Classification rate:  0.1213132999443517  Cost:  5652.601469745488\n",
      "Iteration:  414  Classification rate:  0.1213132999443517  Cost:  5651.292431980084\n",
      "Iteration:  415  Classification rate:  0.1213132999443517  Cost:  5649.984844240495\n",
      "Iteration:  416  Classification rate:  0.12186978297161936  Cost:  5648.678703694717\n",
      "Iteration:  417  Classification rate:  0.12186978297161936  Cost:  5647.374007517831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  418  Classification rate:  0.12242626599888703  Cost:  5646.070752891978\n",
      "Iteration:  419  Classification rate:  0.1229827490261547  Cost:  5644.768937006334\n",
      "Iteration:  420  Classification rate:  0.12353923205342238  Cost:  5643.468557057087\n",
      "Iteration:  421  Classification rate:  0.12353923205342238  Cost:  5642.169610247411\n",
      "Iteration:  422  Classification rate:  0.12353923205342238  Cost:  5640.872093787449\n",
      "Iteration:  423  Classification rate:  0.12409571508069003  Cost:  5639.576004894278\n",
      "Iteration:  424  Classification rate:  0.12409571508069003  Cost:  5638.2813407919\n",
      "Iteration:  425  Classification rate:  0.12409571508069003  Cost:  5636.9880987112065\n",
      "Iteration:  426  Classification rate:  0.12409571508069003  Cost:  5635.69627588996\n",
      "Iteration:  427  Classification rate:  0.1246521981079577  Cost:  5634.405869572774\n",
      "Iteration:  428  Classification rate:  0.1246521981079577  Cost:  5633.116877011082\n",
      "Iteration:  429  Classification rate:  0.12520868113522537  Cost:  5631.829295463125\n",
      "Iteration:  430  Classification rate:  0.12520868113522537  Cost:  5630.54312219392\n",
      "Iteration:  431  Classification rate:  0.12520868113522537  Cost:  5629.258354475242\n",
      "Iteration:  432  Classification rate:  0.12520868113522537  Cost:  5627.974989585602\n",
      "Iteration:  433  Classification rate:  0.12520868113522537  Cost:  5626.693024810217\n",
      "Iteration:  434  Classification rate:  0.12520868113522537  Cost:  5625.412457441002\n",
      "Iteration:  435  Classification rate:  0.12576516416249303  Cost:  5624.1332847765325\n",
      "Iteration:  436  Classification rate:  0.12576516416249303  Cost:  5622.855504122034\n",
      "Iteration:  437  Classification rate:  0.12576516416249303  Cost:  5621.579112789355\n",
      "Iteration:  438  Classification rate:  0.12520868113522537  Cost:  5620.304108096943\n",
      "Iteration:  439  Classification rate:  0.12520868113522537  Cost:  5619.03048736983\n",
      "Iteration:  440  Classification rate:  0.12520868113522537  Cost:  5617.758247939602\n",
      "Iteration:  441  Classification rate:  0.12520868113522537  Cost:  5616.487387144385\n",
      "Iteration:  442  Classification rate:  0.12520868113522537  Cost:  5615.21790232882\n",
      "Iteration:  443  Classification rate:  0.1246521981079577  Cost:  5613.949790844042\n",
      "Iteration:  444  Classification rate:  0.1246521981079577  Cost:  5612.683050047659\n",
      "Iteration:  445  Classification rate:  0.12576516416249303  Cost:  5611.4176773037325\n",
      "Iteration:  446  Classification rate:  0.12576516416249303  Cost:  5610.153669982753\n",
      "Iteration:  447  Classification rate:  0.12576516416249303  Cost:  5608.891025461624\n",
      "Iteration:  448  Classification rate:  0.12576516416249303  Cost:  5607.6297411236355\n",
      "Iteration:  449  Classification rate:  0.12576516416249303  Cost:  5606.369814358448\n",
      "Iteration:  450  Classification rate:  0.12632164718976072  Cost:  5605.111242562072\n",
      "Iteration:  451  Classification rate:  0.12632164718976072  Cost:  5603.854023136842\n",
      "Iteration:  452  Classification rate:  0.12632164718976072  Cost:  5602.598153491406\n",
      "Iteration:  453  Classification rate:  0.12632164718976072  Cost:  5601.343631040694\n",
      "Iteration:  454  Classification rate:  0.12632164718976072  Cost:  5600.090453205903\n",
      "Iteration:  455  Classification rate:  0.12632164718976072  Cost:  5598.8386174144825\n",
      "Iteration:  456  Classification rate:  0.12632164718976072  Cost:  5597.588121100102\n",
      "Iteration:  457  Classification rate:  0.12632164718976072  Cost:  5596.338961702646\n",
      "Iteration:  458  Classification rate:  0.12632164718976072  Cost:  5595.091136668181\n",
      "Iteration:  459  Classification rate:  0.12576516416249303  Cost:  5593.844643448943\n",
      "Iteration:  460  Classification rate:  0.12576516416249303  Cost:  5592.5994795033175\n",
      "Iteration:  461  Classification rate:  0.12576516416249303  Cost:  5591.355642295819\n",
      "Iteration:  462  Classification rate:  0.12576516416249303  Cost:  5590.1131292970695\n",
      "Iteration:  463  Classification rate:  0.12576516416249303  Cost:  5588.871937983783\n",
      "Iteration:  464  Classification rate:  0.12632164718976072  Cost:  5587.632065838743\n",
      "Iteration:  465  Classification rate:  0.12632164718976072  Cost:  5586.393510350789\n",
      "Iteration:  466  Classification rate:  0.12632164718976072  Cost:  5585.156269014788\n",
      "Iteration:  467  Classification rate:  0.12632164718976072  Cost:  5583.920339331626\n",
      "Iteration:  468  Classification rate:  0.12632164718976072  Cost:  5582.68571880818\n",
      "Iteration:  469  Classification rate:  0.12632164718976072  Cost:  5581.452404957308\n",
      "Iteration:  470  Classification rate:  0.12632164718976072  Cost:  5580.220395297824\n",
      "Iteration:  471  Classification rate:  0.12632164718976072  Cost:  5578.989687354481\n",
      "Iteration:  472  Classification rate:  0.12632164718976072  Cost:  5577.760278657952\n",
      "Iteration:  473  Classification rate:  0.12632164718976072  Cost:  5576.532166744817\n",
      "Iteration:  474  Classification rate:  0.12632164718976072  Cost:  5575.305349157536\n",
      "Iteration:  475  Classification rate:  0.12632164718976072  Cost:  5574.079823444436\n",
      "Iteration:  476  Classification rate:  0.12632164718976072  Cost:  5572.855587159695\n",
      "Iteration:  477  Classification rate:  0.12632164718976072  Cost:  5571.632637863316\n",
      "Iteration:  478  Classification rate:  0.12632164718976072  Cost:  5570.410973121122\n",
      "Iteration:  479  Classification rate:  0.12632164718976072  Cost:  5569.1905905047215\n",
      "Iteration:  480  Classification rate:  0.12632164718976072  Cost:  5567.971487591505\n",
      "Iteration:  481  Classification rate:  0.12632164718976072  Cost:  5566.753661964623\n",
      "Iteration:  482  Classification rate:  0.12632164718976072  Cost:  5565.537111212961\n",
      "Iteration:  483  Classification rate:  0.12687813021702837  Cost:  5564.321832931135\n",
      "Iteration:  484  Classification rate:  0.12687813021702837  Cost:  5563.107824719465\n",
      "Iteration:  485  Classification rate:  0.12687813021702837  Cost:  5561.8950841839605\n",
      "Iteration:  486  Classification rate:  0.12687813021702837  Cost:  5560.683608936301\n",
      "Iteration:  487  Classification rate:  0.12687813021702837  Cost:  5559.473396593824\n",
      "Iteration:  488  Classification rate:  0.12743461324429606  Cost:  5558.264444779505\n",
      "Iteration:  489  Classification rate:  0.12743461324429606  Cost:  5557.056751121932\n",
      "Iteration:  490  Classification rate:  0.12743461324429606  Cost:  5555.850313255309\n",
      "Iteration:  491  Classification rate:  0.12743461324429606  Cost:  5554.645128819418\n",
      "Iteration:  492  Classification rate:  0.12743461324429606  Cost:  5553.4411954596135\n",
      "Iteration:  493  Classification rate:  0.12743461324429606  Cost:  5552.238510826806\n",
      "Iteration:  494  Classification rate:  0.12799109627156371  Cost:  5551.037072577439\n",
      "Iteration:  495  Classification rate:  0.12743461324429606  Cost:  5549.836878373479\n",
      "Iteration:  496  Classification rate:  0.12743461324429606  Cost:  5548.637925882395\n",
      "Iteration:  497  Classification rate:  0.12743461324429606  Cost:  5547.440212777144\n",
      "Iteration:  498  Classification rate:  0.12743461324429606  Cost:  5546.243736736153\n",
      "Iteration:  499  Classification rate:  0.12743461324429606  Cost:  5545.048495443308\n",
      "Iteration:  500  Classification rate:  0.12743461324429606  Cost:  5543.8544865879285\n",
      "Iteration:  501  Classification rate:  0.12743461324429606  Cost:  5542.661707864759\n",
      "Iteration:  502  Classification rate:  0.12743461324429606  Cost:  5541.470156973952\n",
      "Iteration:  503  Classification rate:  0.12743461324429606  Cost:  5540.279831621049\n",
      "Iteration:  504  Classification rate:  0.12743461324429606  Cost:  5539.090729516968\n",
      "Iteration:  505  Classification rate:  0.12743461324429606  Cost:  5537.902848377985\n",
      "Iteration:  506  Classification rate:  0.12743461324429606  Cost:  5536.716185925719\n",
      "Iteration:  507  Classification rate:  0.12743461324429606  Cost:  5535.53073988712\n",
      "Iteration:  508  Classification rate:  0.12743461324429606  Cost:  5534.346507994446\n",
      "Iteration:  509  Classification rate:  0.12687813021702837  Cost:  5533.163487985255\n",
      "Iteration:  510  Classification rate:  0.12687813021702837  Cost:  5531.981677602385\n",
      "Iteration:  511  Classification rate:  0.12687813021702837  Cost:  5530.801074593942\n",
      "Iteration:  512  Classification rate:  0.12687813021702837  Cost:  5529.621676713279\n",
      "Iteration:  513  Classification rate:  0.12687813021702837  Cost:  5528.443481718987\n",
      "Iteration:  514  Classification rate:  0.12687813021702837  Cost:  5527.266487374879\n",
      "Iteration:  515  Classification rate:  0.12687813021702837  Cost:  5526.090691449971\n",
      "Iteration:  516  Classification rate:  0.12687813021702837  Cost:  5524.916091718469\n",
      "Iteration:  517  Classification rate:  0.12687813021702837  Cost:  5523.742685959755\n",
      "Iteration:  518  Classification rate:  0.12687813021702837  Cost:  5522.570471958376\n",
      "Iteration:  519  Classification rate:  0.12687813021702837  Cost:  5521.399447504018\n",
      "Iteration:  520  Classification rate:  0.12687813021702837  Cost:  5520.2296103915\n",
      "Iteration:  521  Classification rate:  0.12687813021702837  Cost:  5519.0609584207605\n",
      "Iteration:  522  Classification rate:  0.12687813021702837  Cost:  5517.893489396836\n",
      "Iteration:  523  Classification rate:  0.12687813021702837  Cost:  5516.727201129852\n",
      "Iteration:  524  Classification rate:  0.12687813021702837  Cost:  5515.562091435009\n",
      "Iteration:  525  Classification rate:  0.12687813021702837  Cost:  5514.3981581325625\n",
      "Iteration:  526  Classification rate:  0.12687813021702837  Cost:  5513.235399047811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  527  Classification rate:  0.12687813021702837  Cost:  5512.073812011087\n",
      "Iteration:  528  Classification rate:  0.12687813021702837  Cost:  5510.913394857738\n",
      "Iteration:  529  Classification rate:  0.12687813021702837  Cost:  5509.754145428107\n",
      "Iteration:  530  Classification rate:  0.12687813021702837  Cost:  5508.596061567535\n",
      "Iteration:  531  Classification rate:  0.12687813021702837  Cost:  5507.439141126325\n",
      "Iteration:  532  Classification rate:  0.12687813021702837  Cost:  5506.283381959748\n",
      "Iteration:  533  Classification rate:  0.12687813021702837  Cost:  5505.128781928013\n",
      "Iteration:  534  Classification rate:  0.12743461324429606  Cost:  5503.975338896268\n",
      "Iteration:  535  Classification rate:  0.12743461324429606  Cost:  5502.823050734574\n",
      "Iteration:  536  Classification rate:  0.12687813021702837  Cost:  5501.671915317899\n",
      "Iteration:  537  Classification rate:  0.12687813021702837  Cost:  5500.521930526099\n",
      "Iteration:  538  Classification rate:  0.12632164718976072  Cost:  5499.373094243908\n",
      "Iteration:  539  Classification rate:  0.12632164718976072  Cost:  5498.225404360923\n",
      "Iteration:  540  Classification rate:  0.12632164718976072  Cost:  5497.078858771591\n",
      "Iteration:  541  Classification rate:  0.12632164718976072  Cost:  5495.933455375197\n",
      "Iteration:  542  Classification rate:  0.12632164718976072  Cost:  5494.789192075845\n",
      "Iteration:  543  Classification rate:  0.12632164718976072  Cost:  5493.646066782454\n",
      "Iteration:  544  Classification rate:  0.12632164718976072  Cost:  5492.504077408736\n",
      "Iteration:  545  Classification rate:  0.12576516416249303  Cost:  5491.363221873187\n",
      "Iteration:  546  Classification rate:  0.12576516416249303  Cost:  5490.223498099073\n",
      "Iteration:  547  Classification rate:  0.12520868113522537  Cost:  5489.08490401442\n",
      "Iteration:  548  Classification rate:  0.12520868113522537  Cost:  5487.947437551993\n",
      "Iteration:  549  Classification rate:  0.12520868113522537  Cost:  5486.811096649291\n",
      "Iteration:  550  Classification rate:  0.12520868113522537  Cost:  5485.675879248534\n",
      "Iteration:  551  Classification rate:  0.12520868113522537  Cost:  5484.5417832966405\n",
      "Iteration:  552  Classification rate:  0.12520868113522537  Cost:  5483.408806745228\n",
      "Iteration:  553  Classification rate:  0.12520868113522537  Cost:  5482.27694755059\n",
      "Iteration:  554  Classification rate:  0.12520868113522537  Cost:  5481.146203673691\n",
      "Iteration:  555  Classification rate:  0.12520868113522537  Cost:  5480.016573080145\n",
      "Iteration:  556  Classification rate:  0.12520868113522537  Cost:  5478.88805374021\n",
      "Iteration:  557  Classification rate:  0.12520868113522537  Cost:  5477.760643628775\n",
      "Iteration:  558  Classification rate:  0.12520868113522537  Cost:  5476.634340725345\n",
      "Iteration:  559  Classification rate:  0.12576516416249303  Cost:  5475.509143014028\n",
      "Iteration:  560  Classification rate:  0.12576516416249303  Cost:  5474.385048483527\n",
      "Iteration:  561  Classification rate:  0.12576516416249303  Cost:  5473.262055127121\n",
      "Iteration:  562  Classification rate:  0.12520868113522537  Cost:  5472.14016094266\n",
      "Iteration:  563  Classification rate:  0.12576516416249303  Cost:  5471.019363932547\n",
      "Iteration:  564  Classification rate:  0.12576516416249303  Cost:  5469.8996621037295\n",
      "Iteration:  565  Classification rate:  0.12576516416249303  Cost:  5468.7810534676855\n",
      "Iteration:  566  Classification rate:  0.12576516416249303  Cost:  5467.663536040412\n",
      "Iteration:  567  Classification rate:  0.12576516416249303  Cost:  5466.547107842411\n",
      "Iteration:  568  Classification rate:  0.12632164718976072  Cost:  5465.431766898684\n",
      "Iteration:  569  Classification rate:  0.12632164718976072  Cost:  5464.317511238711\n",
      "Iteration:  570  Classification rate:  0.12632164718976072  Cost:  5463.204338896446\n",
      "Iteration:  571  Classification rate:  0.12632164718976072  Cost:  5462.0922479103\n",
      "Iteration:  572  Classification rate:  0.12632164718976072  Cost:  5460.981236323133\n",
      "Iteration:  573  Classification rate:  0.12632164718976072  Cost:  5459.871302182242\n",
      "Iteration:  574  Classification rate:  0.12632164718976072  Cost:  5458.7624435393445\n",
      "Iteration:  575  Classification rate:  0.12632164718976072  Cost:  5457.654658450574\n",
      "Iteration:  576  Classification rate:  0.12632164718976072  Cost:  5456.547944976464\n",
      "Iteration:  577  Classification rate:  0.12632164718976072  Cost:  5455.442301181939\n",
      "Iteration:  578  Classification rate:  0.12632164718976072  Cost:  5454.3377251362945\n",
      "Iteration:  579  Classification rate:  0.12632164718976072  Cost:  5453.234214913202\n",
      "Iteration:  580  Classification rate:  0.12632164718976072  Cost:  5452.13176859068\n",
      "Iteration:  581  Classification rate:  0.12632164718976072  Cost:  5451.030384251097\n",
      "Iteration:  582  Classification rate:  0.12632164718976072  Cost:  5449.93005998115\n",
      "Iteration:  583  Classification rate:  0.12632164718976072  Cost:  5448.830793871858\n",
      "Iteration:  584  Classification rate:  0.12632164718976072  Cost:  5447.732584018551\n",
      "Iteration:  585  Classification rate:  0.12576516416249303  Cost:  5446.635428520856\n",
      "Iteration:  586  Classification rate:  0.12576516416249303  Cost:  5445.5393254826895\n",
      "Iteration:  587  Classification rate:  0.12576516416249303  Cost:  5444.444273012242\n",
      "Iteration:  588  Classification rate:  0.12576516416249303  Cost:  5443.350269221973\n",
      "Iteration:  589  Classification rate:  0.12576516416249303  Cost:  5442.257312228591\n",
      "Iteration:  590  Classification rate:  0.12576516416249303  Cost:  5441.165400153056\n",
      "Iteration:  591  Classification rate:  0.12576516416249303  Cost:  5440.074531120554\n",
      "Iteration:  592  Classification rate:  0.12576516416249303  Cost:  5438.984703260494\n",
      "Iteration:  593  Classification rate:  0.12576516416249303  Cost:  5437.8959147065\n",
      "Iteration:  594  Classification rate:  0.12576516416249303  Cost:  5436.808163596388\n",
      "Iteration:  595  Classification rate:  0.12576516416249303  Cost:  5435.721448072173\n",
      "Iteration:  596  Classification rate:  0.12576516416249303  Cost:  5434.635766280041\n",
      "Iteration:  597  Classification rate:  0.12576516416249303  Cost:  5433.55111637035\n",
      "Iteration:  598  Classification rate:  0.12632164718976072  Cost:  5432.467496497617\n",
      "Iteration:  599  Classification rate:  0.12632164718976072  Cost:  5431.384904820499\n",
      "Iteration:  600  Classification rate:  0.12632164718976072  Cost:  5430.303339501797\n",
      "Iteration:  601  Classification rate:  0.12576516416249303  Cost:  5429.222798708434\n",
      "Iteration:  602  Classification rate:  0.12576516416249303  Cost:  5428.143280611449\n",
      "Iteration:  603  Classification rate:  0.12576516416249303  Cost:  5427.064783385983\n",
      "Iteration:  604  Classification rate:  0.12576516416249303  Cost:  5425.987305211278\n",
      "Iteration:  605  Classification rate:  0.12576516416249303  Cost:  5424.910844270655\n",
      "Iteration:  606  Classification rate:  0.12576516416249303  Cost:  5423.835398751508\n",
      "Iteration:  607  Classification rate:  0.12576516416249303  Cost:  5422.760966845303\n",
      "Iteration:  608  Classification rate:  0.12576516416249303  Cost:  5421.6875467475475\n",
      "Iteration:  609  Classification rate:  0.12576516416249303  Cost:  5420.6151366578015\n",
      "Iteration:  610  Classification rate:  0.12576516416249303  Cost:  5419.543734779657\n",
      "Iteration:  611  Classification rate:  0.12576516416249303  Cost:  5418.4733393207225\n",
      "Iteration:  612  Classification rate:  0.12576516416249303  Cost:  5417.403948492627\n",
      "Iteration:  613  Classification rate:  0.12632164718976072  Cost:  5416.335560510997\n",
      "Iteration:  614  Classification rate:  0.12632164718976072  Cost:  5415.2681735954575\n",
      "Iteration:  615  Classification rate:  0.12632164718976072  Cost:  5414.201785969611\n",
      "Iteration:  616  Classification rate:  0.12632164718976072  Cost:  5413.1363958610345\n",
      "Iteration:  617  Classification rate:  0.12632164718976072  Cost:  5412.072001501273\n",
      "Iteration:  618  Classification rate:  0.12632164718976072  Cost:  5411.008601125815\n",
      "Iteration:  619  Classification rate:  0.12632164718976072  Cost:  5409.946192974104\n",
      "Iteration:  620  Classification rate:  0.12632164718976072  Cost:  5408.884775289509\n",
      "Iteration:  621  Classification rate:  0.12632164718976072  Cost:  5407.824346319324\n",
      "Iteration:  622  Classification rate:  0.12632164718976072  Cost:  5406.76490431476\n",
      "Iteration:  623  Classification rate:  0.12632164718976072  Cost:  5405.706447530934\n",
      "Iteration:  624  Classification rate:  0.12632164718976072  Cost:  5404.648974226853\n",
      "Iteration:  625  Classification rate:  0.12687813021702837  Cost:  5403.592482665413\n",
      "Iteration:  626  Classification rate:  0.12687813021702837  Cost:  5402.536971113383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  627  Classification rate:  0.12687813021702837  Cost:  5401.4824378414\n",
      "Iteration:  628  Classification rate:  0.12687813021702837  Cost:  5400.428881123962\n",
      "Iteration:  629  Classification rate:  0.12743461324429606  Cost:  5399.376299239405\n",
      "Iteration:  630  Classification rate:  0.12799109627156371  Cost:  5398.324690469913\n",
      "Iteration:  631  Classification rate:  0.12799109627156371  Cost:  5397.274053101489\n",
      "Iteration:  632  Classification rate:  0.12799109627156371  Cost:  5396.224385423962\n",
      "Iteration:  633  Classification rate:  0.12799109627156371  Cost:  5395.175685730971\n",
      "Iteration:  634  Classification rate:  0.12799109627156371  Cost:  5394.12795231995\n",
      "Iteration:  635  Classification rate:  0.12799109627156371  Cost:  5393.08118349213\n",
      "Iteration:  636  Classification rate:  0.12799109627156371  Cost:  5392.035377552521\n",
      "Iteration:  637  Classification rate:  0.12799109627156371  Cost:  5390.990532809906\n",
      "Iteration:  638  Classification rate:  0.12799109627156371  Cost:  5389.946647576833\n",
      "Iteration:  639  Classification rate:  0.12743461324429606  Cost:  5388.903720169604\n",
      "Iteration:  640  Classification rate:  0.12743461324429606  Cost:  5387.861748908266\n",
      "Iteration:  641  Classification rate:  0.12743461324429606  Cost:  5386.820732116604\n",
      "Iteration:  642  Classification rate:  0.12799109627156371  Cost:  5385.78066812213\n",
      "Iteration:  643  Classification rate:  0.12799109627156371  Cost:  5384.7415552560715\n",
      "Iteration:  644  Classification rate:  0.12799109627156371  Cost:  5383.703391853369\n",
      "Iteration:  645  Classification rate:  0.12799109627156371  Cost:  5382.666176252664\n",
      "Iteration:  646  Classification rate:  0.12743461324429606  Cost:  5381.629906796285\n",
      "Iteration:  647  Classification rate:  0.12743461324429606  Cost:  5380.594581830248\n",
      "Iteration:  648  Classification rate:  0.12743461324429606  Cost:  5379.56019970424\n",
      "Iteration:  649  Classification rate:  0.12743461324429606  Cost:  5378.526758771614\n",
      "Iteration:  650  Classification rate:  0.12743461324429606  Cost:  5377.49425738938\n",
      "Iteration:  651  Classification rate:  0.12743461324429606  Cost:  5376.4626939181935\n",
      "Iteration:  652  Classification rate:  0.12743461324429606  Cost:  5375.432066722351\n",
      "Iteration:  653  Classification rate:  0.12743461324429606  Cost:  5374.402374169777\n",
      "Iteration:  654  Classification rate:  0.12743461324429606  Cost:  5373.373614632022\n",
      "Iteration:  655  Classification rate:  0.12687813021702837  Cost:  5372.345786484243\n",
      "Iteration:  656  Classification rate:  0.12687813021702837  Cost:  5371.318888105204\n",
      "Iteration:  657  Classification rate:  0.12687813021702837  Cost:  5370.292917877265\n",
      "Iteration:  658  Classification rate:  0.12687813021702837  Cost:  5369.267874186372\n",
      "Iteration:  659  Classification rate:  0.12687813021702837  Cost:  5368.243755422051\n",
      "Iteration:  660  Classification rate:  0.12687813021702837  Cost:  5367.220559977399\n",
      "Iteration:  661  Classification rate:  0.12687813021702837  Cost:  5366.1982862490695\n",
      "Iteration:  662  Classification rate:  0.12687813021702837  Cost:  5365.176932637274\n",
      "Iteration:  663  Classification rate:  0.12687813021702837  Cost:  5364.156497545767\n",
      "Iteration:  664  Classification rate:  0.12687813021702837  Cost:  5363.136979381838\n",
      "Iteration:  665  Classification rate:  0.12687813021702837  Cost:  5362.118376556308\n",
      "Iteration:  666  Classification rate:  0.12687813021702837  Cost:  5361.100687483514\n",
      "Iteration:  667  Classification rate:  0.12687813021702837  Cost:  5360.0839105813075\n",
      "Iteration:  668  Classification rate:  0.12687813021702837  Cost:  5359.06804427104\n",
      "Iteration:  669  Classification rate:  0.12743461324429606  Cost:  5358.053086977558\n",
      "Iteration:  670  Classification rate:  0.12743461324429606  Cost:  5357.039037129201\n",
      "Iteration:  671  Classification rate:  0.12743461324429606  Cost:  5356.0258931577755\n",
      "Iteration:  672  Classification rate:  0.12743461324429606  Cost:  5355.013653498567\n",
      "Iteration:  673  Classification rate:  0.12743461324429606  Cost:  5354.002316590322\n",
      "Iteration:  674  Classification rate:  0.12743461324429606  Cost:  5352.991880875238\n",
      "Iteration:  675  Classification rate:  0.12743461324429606  Cost:  5351.98234479896\n",
      "Iteration:  676  Classification rate:  0.12687813021702837  Cost:  5350.973706810573\n",
      "Iteration:  677  Classification rate:  0.12687813021702837  Cost:  5349.965965362589\n",
      "Iteration:  678  Classification rate:  0.12687813021702837  Cost:  5348.959118910941\n",
      "Iteration:  679  Classification rate:  0.12687813021702837  Cost:  5347.953165914977\n",
      "Iteration:  680  Classification rate:  0.12687813021702837  Cost:  5346.948104837453\n",
      "Iteration:  681  Classification rate:  0.12687813021702837  Cost:  5345.943934144522\n",
      "Iteration:  682  Classification rate:  0.12687813021702837  Cost:  5344.940652305724\n",
      "Iteration:  683  Classification rate:  0.12687813021702837  Cost:  5343.938257793985\n",
      "Iteration:  684  Classification rate:  0.12687813021702837  Cost:  5342.936749085602\n",
      "Iteration:  685  Classification rate:  0.12687813021702837  Cost:  5341.936124660242\n",
      "Iteration:  686  Classification rate:  0.12687813021702837  Cost:  5340.936383000928\n",
      "Iteration:  687  Classification rate:  0.12687813021702837  Cost:  5339.937522594034\n",
      "Iteration:  688  Classification rate:  0.12687813021702837  Cost:  5338.93954192928\n",
      "Iteration:  689  Classification rate:  0.12687813021702837  Cost:  5337.942439499717\n",
      "Iteration:  690  Classification rate:  0.12687813021702837  Cost:  5336.946213801726\n",
      "Iteration:  691  Classification rate:  0.12687813021702837  Cost:  5335.950863335008\n",
      "Iteration:  692  Classification rate:  0.12687813021702837  Cost:  5334.956386602578\n",
      "Iteration:  693  Classification rate:  0.12687813021702837  Cost:  5333.9627821107515\n",
      "Iteration:  694  Classification rate:  0.12687813021702837  Cost:  5332.970048369144\n",
      "Iteration:  695  Classification rate:  0.12687813021702837  Cost:  5331.97818389066\n",
      "Iteration:  696  Classification rate:  0.12687813021702837  Cost:  5330.9871871914875\n",
      "Iteration:  697  Classification rate:  0.12687813021702837  Cost:  5329.997056791084\n",
      "Iteration:  698  Classification rate:  0.12687813021702837  Cost:  5329.007791212181\n",
      "Iteration:  699  Classification rate:  0.12743461324429606  Cost:  5328.01938898076\n",
      "Iteration:  700  Classification rate:  0.12743461324429606  Cost:  5327.031848626063\n",
      "Iteration:  701  Classification rate:  0.12743461324429606  Cost:  5326.045168680575\n",
      "Iteration:  702  Classification rate:  0.12799109627156371  Cost:  5325.059347680012\n",
      "Iteration:  703  Classification rate:  0.12799109627156371  Cost:  5324.074384163326\n",
      "Iteration:  704  Classification rate:  0.12799109627156371  Cost:  5323.09027667269\n",
      "Iteration:  705  Classification rate:  0.12799109627156371  Cost:  5322.107023753488\n",
      "Iteration:  706  Classification rate:  0.12743461324429606  Cost:  5321.124623954314\n",
      "Iteration:  707  Classification rate:  0.12743461324429606  Cost:  5320.143075826965\n",
      "Iteration:  708  Classification rate:  0.12743461324429606  Cost:  5319.162377926428\n",
      "Iteration:  709  Classification rate:  0.12743461324429606  Cost:  5318.182528810873\n",
      "Iteration:  710  Classification rate:  0.12743461324429606  Cost:  5317.203527041654\n",
      "Iteration:  711  Classification rate:  0.12687813021702837  Cost:  5316.225371183291\n",
      "Iteration:  712  Classification rate:  0.12687813021702837  Cost:  5315.248059803472\n",
      "Iteration:  713  Classification rate:  0.12687813021702837  Cost:  5314.271591473038\n",
      "Iteration:  714  Classification rate:  0.12687813021702837  Cost:  5313.295964765983\n",
      "Iteration:  715  Classification rate:  0.12687813021702837  Cost:  5312.321178259441\n",
      "Iteration:  716  Classification rate:  0.12687813021702837  Cost:  5311.347230533682\n",
      "Iteration:  717  Classification rate:  0.12687813021702837  Cost:  5310.374120172103\n",
      "Iteration:  718  Classification rate:  0.12687813021702837  Cost:  5309.401845761226\n",
      "Iteration:  719  Classification rate:  0.12687813021702837  Cost:  5308.430405890681\n",
      "Iteration:  720  Classification rate:  0.12687813021702837  Cost:  5307.45979915321\n",
      "Iteration:  721  Classification rate:  0.12687813021702837  Cost:  5306.490024144653\n",
      "Iteration:  722  Classification rate:  0.12687813021702837  Cost:  5305.521079463942\n",
      "Iteration:  723  Classification rate:  0.12687813021702837  Cost:  5304.5529637130985\n",
      "Iteration:  724  Classification rate:  0.12687813021702837  Cost:  5303.585675497218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  725  Classification rate:  0.12687813021702837  Cost:  5302.619213424471\n",
      "Iteration:  726  Classification rate:  0.12687813021702837  Cost:  5301.653576106093\n",
      "Iteration:  727  Classification rate:  0.12687813021702837  Cost:  5300.688762156378\n",
      "Iteration:  728  Classification rate:  0.12687813021702837  Cost:  5299.72477019267\n",
      "Iteration:  729  Classification rate:  0.12687813021702837  Cost:  5298.761598835357\n",
      "Iteration:  730  Classification rate:  0.12687813021702837  Cost:  5297.799246707868\n",
      "Iteration:  731  Classification rate:  0.12687813021702837  Cost:  5296.83771243666\n",
      "Iteration:  732  Classification rate:  0.12743461324429606  Cost:  5295.876994651214\n",
      "Iteration:  733  Classification rate:  0.12743461324429606  Cost:  5294.917091984029\n",
      "Iteration:  734  Classification rate:  0.12743461324429606  Cost:  5293.958003070612\n",
      "Iteration:  735  Classification rate:  0.12743461324429606  Cost:  5292.999726549481\n",
      "Iteration:  736  Classification rate:  0.12743461324429606  Cost:  5292.042261062143\n",
      "Iteration:  737  Classification rate:  0.12743461324429606  Cost:  5291.085605253099\n",
      "Iteration:  738  Classification rate:  0.12743461324429606  Cost:  5290.1297577698315\n",
      "Iteration:  739  Classification rate:  0.12743461324429606  Cost:  5289.174717262803\n",
      "Iteration:  740  Classification rate:  0.12743461324429606  Cost:  5288.220482385443\n",
      "Iteration:  741  Classification rate:  0.12743461324429606  Cost:  5287.267051794147\n",
      "Iteration:  742  Classification rate:  0.12743461324429606  Cost:  5286.314424148267\n",
      "Iteration:  743  Classification rate:  0.12743461324429606  Cost:  5285.362598110105\n",
      "Iteration:  744  Classification rate:  0.12743461324429606  Cost:  5284.411572344905\n",
      "Iteration:  745  Classification rate:  0.12743461324429606  Cost:  5283.461345520851\n",
      "Iteration:  746  Classification rate:  0.12743461324429606  Cost:  5282.511916309058\n",
      "Iteration:  747  Classification rate:  0.12743461324429606  Cost:  5281.563283383563\n",
      "Iteration:  748  Classification rate:  0.12743461324429606  Cost:  5280.615445421323\n",
      "Iteration:  749  Classification rate:  0.12743461324429606  Cost:  5279.6684011022035\n",
      "Iteration:  750  Classification rate:  0.12743461324429606  Cost:  5278.722149108977\n",
      "Iteration:  751  Classification rate:  0.12743461324429606  Cost:  5277.776688127314\n",
      "Iteration:  752  Classification rate:  0.12743461324429606  Cost:  5276.832016845775\n",
      "Iteration:  753  Classification rate:  0.12743461324429606  Cost:  5275.888133955807\n",
      "Iteration:  754  Classification rate:  0.12743461324429606  Cost:  5274.945038151737\n",
      "Iteration:  755  Classification rate:  0.12743461324429606  Cost:  5274.002728130766\n",
      "Iteration:  756  Classification rate:  0.12743461324429606  Cost:  5273.061202592956\n",
      "Iteration:  757  Classification rate:  0.12743461324429606  Cost:  5272.120460241233\n",
      "Iteration:  758  Classification rate:  0.12743461324429606  Cost:  5271.180499781377\n",
      "Iteration:  759  Classification rate:  0.12743461324429606  Cost:  5270.241319922014\n",
      "Iteration:  760  Classification rate:  0.12743461324429606  Cost:  5269.302919374607\n",
      "Iteration:  761  Classification rate:  0.12743461324429606  Cost:  5268.3652968534625\n",
      "Iteration:  762  Classification rate:  0.12743461324429606  Cost:  5267.428451075709\n",
      "Iteration:  763  Classification rate:  0.12743461324429606  Cost:  5266.4923807612995\n",
      "Iteration:  764  Classification rate:  0.12743461324429606  Cost:  5265.557084633001\n",
      "Iteration:  765  Classification rate:  0.12743461324429606  Cost:  5264.622561416391\n",
      "Iteration:  766  Classification rate:  0.12799109627156371  Cost:  5263.688809839853\n",
      "Iteration:  767  Classification rate:  0.12799109627156371  Cost:  5262.755828634567\n",
      "Iteration:  768  Classification rate:  0.12799109627156371  Cost:  5261.823616534498\n",
      "Iteration:  769  Classification rate:  0.12799109627156371  Cost:  5260.892172276405\n",
      "Iteration:  770  Classification rate:  0.12799109627156371  Cost:  5259.961494599824\n",
      "Iteration:  771  Classification rate:  0.12799109627156371  Cost:  5259.0315822470575\n",
      "Iteration:  772  Classification rate:  0.12799109627156371  Cost:  5258.102433963181\n",
      "Iteration:  773  Classification rate:  0.12799109627156371  Cost:  5257.17404849603\n",
      "Iteration:  774  Classification rate:  0.12799109627156371  Cost:  5256.246424596193\n",
      "Iteration:  775  Classification rate:  0.12799109627156371  Cost:  5255.319561017005\n",
      "Iteration:  776  Classification rate:  0.12799109627156371  Cost:  5254.393456514548\n",
      "Iteration:  777  Classification rate:  0.1285475792988314  Cost:  5253.4681098476385\n",
      "Iteration:  778  Classification rate:  0.12799109627156371  Cost:  5252.543519777821\n",
      "Iteration:  779  Classification rate:  0.12799109627156371  Cost:  5251.619685069369\n",
      "Iteration:  780  Classification rate:  0.12799109627156371  Cost:  5250.6966044892715\n",
      "Iteration:  781  Classification rate:  0.1285475792988314  Cost:  5249.774276807229\n",
      "Iteration:  782  Classification rate:  0.1285475792988314  Cost:  5248.852700795654\n",
      "Iteration:  783  Classification rate:  0.1285475792988314  Cost:  5247.931875229651\n",
      "Iteration:  784  Classification rate:  0.1285475792988314  Cost:  5247.011798887028\n",
      "Iteration:  785  Classification rate:  0.1285475792988314  Cost:  5246.092470548276\n",
      "Iteration:  786  Classification rate:  0.1285475792988314  Cost:  5245.173888996571\n",
      "Iteration:  787  Classification rate:  0.1285475792988314  Cost:  5244.256053017767\n",
      "Iteration:  788  Classification rate:  0.1285475792988314  Cost:  5243.338961400387\n",
      "Iteration:  789  Classification rate:  0.1285475792988314  Cost:  5242.422612935624\n",
      "Iteration:  790  Classification rate:  0.1285475792988314  Cost:  5241.5070064173215\n",
      "Iteration:  791  Classification rate:  0.1285475792988314  Cost:  5240.592140641986\n",
      "Iteration:  792  Classification rate:  0.1285475792988314  Cost:  5239.678014408769\n",
      "Iteration:  793  Classification rate:  0.1285475792988314  Cost:  5238.764626519463\n",
      "Iteration:  794  Classification rate:  0.1285475792988314  Cost:  5237.851975778496\n",
      "Iteration:  795  Classification rate:  0.1285475792988314  Cost:  5236.940060992931\n",
      "Iteration:  796  Classification rate:  0.1285475792988314  Cost:  5236.028880972452\n",
      "Iteration:  797  Classification rate:  0.1285475792988314  Cost:  5235.118434529364\n",
      "Iteration:  798  Classification rate:  0.12910406232609906  Cost:  5234.208720478585\n",
      "Iteration:  799  Classification rate:  0.12910406232609906  Cost:  5233.299737637639\n",
      "Iteration:  800  Classification rate:  0.12910406232609906  Cost:  5232.391484826657\n",
      "Iteration:  801  Classification rate:  0.12910406232609906  Cost:  5231.4839608683615\n",
      "Iteration:  802  Classification rate:  0.12910406232609906  Cost:  5230.577164588069\n",
      "Iteration:  803  Classification rate:  0.12910406232609906  Cost:  5229.67109481368\n",
      "Iteration:  804  Classification rate:  0.12910406232609906  Cost:  5228.765750375674\n",
      "Iteration:  805  Classification rate:  0.12910406232609906  Cost:  5227.861130107106\n",
      "Iteration:  806  Classification rate:  0.12910406232609906  Cost:  5226.957232843598\n",
      "Iteration:  807  Classification rate:  0.12910406232609906  Cost:  5226.054057423337\n",
      "Iteration:  808  Classification rate:  0.12910406232609906  Cost:  5225.151602687064\n",
      "Iteration:  809  Classification rate:  0.12910406232609906  Cost:  5224.249867478075\n",
      "Iteration:  810  Classification rate:  0.12910406232609906  Cost:  5223.34885064221\n",
      "Iteration:  811  Classification rate:  0.12910406232609906  Cost:  5222.448551027849\n",
      "Iteration:  812  Classification rate:  0.12910406232609906  Cost:  5221.548967485912\n",
      "Iteration:  813  Classification rate:  0.12910406232609906  Cost:  5220.65009886984\n",
      "Iteration:  814  Classification rate:  0.12910406232609906  Cost:  5219.751944035608\n",
      "Iteration:  815  Classification rate:  0.12910406232609906  Cost:  5218.854501841703\n",
      "Iteration:  816  Classification rate:  0.12910406232609906  Cost:  5217.957771149126\n",
      "Iteration:  817  Classification rate:  0.12910406232609906  Cost:  5217.061750821389\n",
      "Iteration:  818  Classification rate:  0.12910406232609906  Cost:  5216.166439724503\n",
      "Iteration:  819  Classification rate:  0.12910406232609906  Cost:  5215.271836726978\n",
      "Iteration:  820  Classification rate:  0.12910406232609906  Cost:  5214.377940699816\n",
      "Iteration:  821  Classification rate:  0.12910406232609906  Cost:  5213.484750516501\n",
      "Iteration:  822  Classification rate:  0.12910406232609906  Cost:  5212.592265053004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  823  Classification rate:  0.1296605453533667  Cost:  5211.700483187766\n",
      "Iteration:  824  Classification rate:  0.1296605453533667  Cost:  5210.809403801701\n",
      "Iteration:  825  Classification rate:  0.1296605453533667  Cost:  5209.919025778188\n",
      "Iteration:  826  Classification rate:  0.1296605453533667  Cost:  5209.029348003064\n",
      "Iteration:  827  Classification rate:  0.1296605453533667  Cost:  5208.140369364621\n",
      "Iteration:  828  Classification rate:  0.1296605453533667  Cost:  5207.252088753597\n",
      "Iteration:  829  Classification rate:  0.1296605453533667  Cost:  5206.3645050631785\n",
      "Iteration:  830  Classification rate:  0.1296605453533667  Cost:  5205.477617188987\n",
      "Iteration:  831  Classification rate:  0.1296605453533667  Cost:  5204.591424029078\n",
      "Iteration:  832  Classification rate:  0.1296605453533667  Cost:  5203.705924483932\n",
      "Iteration:  833  Classification rate:  0.1296605453533667  Cost:  5202.821117456458\n",
      "Iteration:  834  Classification rate:  0.1296605453533667  Cost:  5201.937001851975\n",
      "Iteration:  835  Classification rate:  0.1296605453533667  Cost:  5201.053576578223\n",
      "Iteration:  836  Classification rate:  0.1296605453533667  Cost:  5200.170840545338\n",
      "Iteration:  837  Classification rate:  0.1302170283806344  Cost:  5199.288792665866\n",
      "Iteration:  838  Classification rate:  0.1302170283806344  Cost:  5198.407431854746\n",
      "Iteration:  839  Classification rate:  0.1302170283806344  Cost:  5197.52675702931\n",
      "Iteration:  840  Classification rate:  0.1302170283806344  Cost:  5196.646767109276\n",
      "Iteration:  841  Classification rate:  0.1302170283806344  Cost:  5195.76746101674\n",
      "Iteration:  842  Classification rate:  0.13077351140790205  Cost:  5194.88883767618\n",
      "Iteration:  843  Classification rate:  0.13077351140790205  Cost:  5194.010896014437\n",
      "Iteration:  844  Classification rate:  0.13077351140790205  Cost:  5193.1336349607245\n",
      "Iteration:  845  Classification rate:  0.13077351140790205  Cost:  5192.257053446615\n",
      "Iteration:  846  Classification rate:  0.13077351140790205  Cost:  5191.381150406033\n",
      "Iteration:  847  Classification rate:  0.13077351140790205  Cost:  5190.50592477526\n",
      "Iteration:  848  Classification rate:  0.13077351140790205  Cost:  5189.631375492915\n",
      "Iteration:  849  Classification rate:  0.13077351140790205  Cost:  5188.757501499964\n",
      "Iteration:  850  Classification rate:  0.13077351140790205  Cost:  5187.884301739705\n",
      "Iteration:  851  Classification rate:  0.13077351140790205  Cost:  5187.011775157768\n",
      "Iteration:  852  Classification rate:  0.13077351140790205  Cost:  5186.139920702109\n",
      "Iteration:  853  Classification rate:  0.13077351140790205  Cost:  5185.268737323001\n",
      "Iteration:  854  Classification rate:  0.13077351140790205  Cost:  5184.398223973035\n",
      "Iteration:  855  Classification rate:  0.13077351140790205  Cost:  5183.528379607113\n",
      "Iteration:  856  Classification rate:  0.13077351140790205  Cost:  5182.65920318244\n",
      "Iteration:  857  Classification rate:  0.13077351140790205  Cost:  5181.790693658526\n",
      "Iteration:  858  Classification rate:  0.13077351140790205  Cost:  5180.922849997169\n",
      "Iteration:  859  Classification rate:  0.13077351140790205  Cost:  5180.055671162466\n",
      "Iteration:  860  Classification rate:  0.1302170283806344  Cost:  5179.189156120794\n",
      "Iteration:  861  Classification rate:  0.1302170283806344  Cost:  5178.323303840813\n",
      "Iteration:  862  Classification rate:  0.1302170283806344  Cost:  5177.458113293459\n",
      "Iteration:  863  Classification rate:  0.1302170283806344  Cost:  5176.593583451941\n",
      "Iteration:  864  Classification rate:  0.1302170283806344  Cost:  5175.729713291731\n",
      "Iteration:  865  Classification rate:  0.1302170283806344  Cost:  5174.866501790563\n",
      "Iteration:  866  Classification rate:  0.1302170283806344  Cost:  5174.003947928425\n",
      "Iteration:  867  Classification rate:  0.1296605453533667  Cost:  5173.142050687567\n",
      "Iteration:  868  Classification rate:  0.1296605453533667  Cost:  5172.280809052472\n",
      "Iteration:  869  Classification rate:  0.1296605453533667  Cost:  5171.420222009876\n",
      "Iteration:  870  Classification rate:  0.1296605453533667  Cost:  5170.560288548745\n",
      "Iteration:  871  Classification rate:  0.1296605453533667  Cost:  5169.701007660283\n",
      "Iteration:  872  Classification rate:  0.1296605453533667  Cost:  5168.842378337917\n",
      "Iteration:  873  Classification rate:  0.1296605453533667  Cost:  5167.984399577302\n",
      "Iteration:  874  Classification rate:  0.1296605453533667  Cost:  5167.127070376305\n",
      "Iteration:  875  Classification rate:  0.1296605453533667  Cost:  5166.270389735011\n",
      "Iteration:  876  Classification rate:  0.1296605453533667  Cost:  5165.414356655713\n",
      "Iteration:  877  Classification rate:  0.1296605453533667  Cost:  5164.558970142908\n",
      "Iteration:  878  Classification rate:  0.1296605453533667  Cost:  5163.704229203292\n",
      "Iteration:  879  Classification rate:  0.1296605453533667  Cost:  5162.850132845751\n",
      "Iteration:  880  Classification rate:  0.1296605453533667  Cost:  5161.996680081373\n",
      "Iteration:  881  Classification rate:  0.1296605453533667  Cost:  5161.143869923417\n",
      "Iteration:  882  Classification rate:  0.1296605453533667  Cost:  5160.29170138733\n",
      "Iteration:  883  Classification rate:  0.1296605453533667  Cost:  5159.440173490736\n",
      "Iteration:  884  Classification rate:  0.1302170283806344  Cost:  5158.589285253427\n",
      "Iteration:  885  Classification rate:  0.1302170283806344  Cost:  5157.739035697363\n",
      "Iteration:  886  Classification rate:  0.1302170283806344  Cost:  5156.889423846665\n",
      "Iteration:  887  Classification rate:  0.1302170283806344  Cost:  5156.040448727612\n",
      "Iteration:  888  Classification rate:  0.1302170283806344  Cost:  5155.192109368637\n",
      "Iteration:  889  Classification rate:  0.1302170283806344  Cost:  5154.344404800318\n",
      "Iteration:  890  Classification rate:  0.1302170283806344  Cost:  5153.49733405538\n",
      "Iteration:  891  Classification rate:  0.1302170283806344  Cost:  5152.650896168685\n",
      "Iteration:  892  Classification rate:  0.1302170283806344  Cost:  5151.80509017723\n",
      "Iteration:  893  Classification rate:  0.1302170283806344  Cost:  5150.959915120143\n",
      "Iteration:  894  Classification rate:  0.1302170283806344  Cost:  5150.115370038675\n",
      "Iteration:  895  Classification rate:  0.1302170283806344  Cost:  5149.271453976198\n",
      "Iteration:  896  Classification rate:  0.1302170283806344  Cost:  5148.428165978203\n",
      "Iteration:  897  Classification rate:  0.1296605453533667  Cost:  5147.58550509229\n",
      "Iteration:  898  Classification rate:  0.1302170283806344  Cost:  5146.743470368169\n",
      "Iteration:  899  Classification rate:  0.1302170283806344  Cost:  5145.902060857649\n",
      "Iteration:  900  Classification rate:  0.1296605453533667  Cost:  5145.06127561464\n",
      "Iteration:  901  Classification rate:  0.1296605453533667  Cost:  5144.221113695146\n",
      "Iteration:  902  Classification rate:  0.1296605453533667  Cost:  5143.381574157262\n",
      "Iteration:  903  Classification rate:  0.1296605453533667  Cost:  5142.5426560611595\n",
      "Iteration:  904  Classification rate:  0.1296605453533667  Cost:  5141.704358469101\n",
      "Iteration:  905  Classification rate:  0.1296605453533667  Cost:  5140.866680445421\n",
      "Iteration:  906  Classification rate:  0.1296605453533667  Cost:  5140.02962105652\n",
      "Iteration:  907  Classification rate:  0.1296605453533667  Cost:  5139.193179370877\n",
      "Iteration:  908  Classification rate:  0.1296605453533667  Cost:  5138.357354459025\n",
      "Iteration:  909  Classification rate:  0.1296605453533667  Cost:  5137.52214539356\n",
      "Iteration:  910  Classification rate:  0.1296605453533667  Cost:  5136.687551249128\n",
      "Iteration:  911  Classification rate:  0.1296605453533667  Cost:  5135.853571102427\n",
      "Iteration:  912  Classification rate:  0.1296605453533667  Cost:  5135.0202040322\n",
      "Iteration:  913  Classification rate:  0.1296605453533667  Cost:  5134.187449119233\n",
      "Iteration:  914  Classification rate:  0.1296605453533667  Cost:  5133.3553054463455\n",
      "Iteration:  915  Classification rate:  0.1296605453533667  Cost:  5132.5237720983905\n",
      "Iteration:  916  Classification rate:  0.1296605453533667  Cost:  5131.692848162248\n",
      "Iteration:  917  Classification rate:  0.1296605453533667  Cost:  5130.8625327268255\n",
      "Iteration:  918  Classification rate:  0.1296605453533667  Cost:  5130.032824883045\n",
      "Iteration:  919  Classification rate:  0.1296605453533667  Cost:  5129.2037237238455\n",
      "Iteration:  920  Classification rate:  0.1296605453533667  Cost:  5128.375228344176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  921  Classification rate:  0.1296605453533667  Cost:  5127.547337840993\n",
      "Iteration:  922  Classification rate:  0.1296605453533667  Cost:  5126.7200513132575\n",
      "Iteration:  923  Classification rate:  0.1296605453533667  Cost:  5125.893367861923\n",
      "Iteration:  924  Classification rate:  0.1302170283806344  Cost:  5125.067286589941\n",
      "Iteration:  925  Classification rate:  0.1302170283806344  Cost:  5124.241806602251\n",
      "Iteration:  926  Classification rate:  0.1302170283806344  Cost:  5123.416927005776\n",
      "Iteration:  927  Classification rate:  0.1302170283806344  Cost:  5122.592646909425\n",
      "Iteration:  928  Classification rate:  0.1302170283806344  Cost:  5121.7689654240785\n",
      "Iteration:  929  Classification rate:  0.1302170283806344  Cost:  5120.945881662592\n",
      "Iteration:  930  Classification rate:  0.1302170283806344  Cost:  5120.12339473979\n",
      "Iteration:  931  Classification rate:  0.1302170283806344  Cost:  5119.301503772459\n",
      "Iteration:  932  Classification rate:  0.1302170283806344  Cost:  5118.480207879349\n",
      "Iteration:  933  Classification rate:  0.1302170283806344  Cost:  5117.659506181162\n",
      "Iteration:  934  Classification rate:  0.1302170283806344  Cost:  5116.839397800555\n",
      "Iteration:  935  Classification rate:  0.1302170283806344  Cost:  5116.019881862133\n",
      "Iteration:  936  Classification rate:  0.1302170283806344  Cost:  5115.200957492439\n",
      "Iteration:  937  Classification rate:  0.1302170283806344  Cost:  5114.382623819962\n",
      "Iteration:  938  Classification rate:  0.1302170283806344  Cost:  5113.56487997512\n",
      "Iteration:  939  Classification rate:  0.1302170283806344  Cost:  5112.74772509027\n",
      "Iteration:  940  Classification rate:  0.1302170283806344  Cost:  5111.931158299688\n",
      "Iteration:  941  Classification rate:  0.1302170283806344  Cost:  5111.115178739578\n",
      "Iteration:  942  Classification rate:  0.1302170283806344  Cost:  5110.2997855480635\n",
      "Iteration:  943  Classification rate:  0.1302170283806344  Cost:  5109.484977865176\n",
      "Iteration:  944  Classification rate:  0.1302170283806344  Cost:  5108.670754832865\n",
      "Iteration:  945  Classification rate:  0.1302170283806344  Cost:  5107.857115594983\n",
      "Iteration:  946  Classification rate:  0.1302170283806344  Cost:  5107.044059297287\n",
      "Iteration:  947  Classification rate:  0.1302170283806344  Cost:  5106.231585087431\n",
      "Iteration:  948  Classification rate:  0.1302170283806344  Cost:  5105.419692114965\n",
      "Iteration:  949  Classification rate:  0.1296605453533667  Cost:  5104.608379531329\n",
      "Iteration:  950  Classification rate:  0.1296605453533667  Cost:  5103.7976464898475\n",
      "Iteration:  951  Classification rate:  0.1296605453533667  Cost:  5102.98749214573\n",
      "Iteration:  952  Classification rate:  0.1296605453533667  Cost:  5102.177915656065\n",
      "Iteration:  953  Classification rate:  0.12910406232609906  Cost:  5101.368916179814\n",
      "Iteration:  954  Classification rate:  0.12910406232609906  Cost:  5100.5604928778075\n",
      "Iteration:  955  Classification rate:  0.12910406232609906  Cost:  5099.752644912747\n",
      "Iteration:  956  Classification rate:  0.12910406232609906  Cost:  5098.945371449196\n",
      "Iteration:  957  Classification rate:  0.12910406232609906  Cost:  5098.138671653571\n",
      "Iteration:  958  Classification rate:  0.12910406232609906  Cost:  5097.332544694147\n",
      "Iteration:  959  Classification rate:  0.12910406232609906  Cost:  5096.526989741053\n",
      "Iteration:  960  Classification rate:  0.12910406232609906  Cost:  5095.72200596626\n",
      "Iteration:  961  Classification rate:  0.12910406232609906  Cost:  5094.917592543585\n",
      "Iteration:  962  Classification rate:  0.12910406232609906  Cost:  5094.113748648683\n",
      "Iteration:  963  Classification rate:  0.12910406232609906  Cost:  5093.310473459043\n",
      "Iteration:  964  Classification rate:  0.12910406232609906  Cost:  5092.507766153988\n",
      "Iteration:  965  Classification rate:  0.12799109627156371  Cost:  5091.705625914666\n",
      "Iteration:  966  Classification rate:  0.1285475792988314  Cost:  5090.90405192405\n",
      "Iteration:  967  Classification rate:  0.12910406232609906  Cost:  5090.103043366931\n",
      "Iteration:  968  Classification rate:  0.1296605453533667  Cost:  5089.302599429919\n",
      "Iteration:  969  Classification rate:  0.1296605453533667  Cost:  5088.502719301432\n",
      "Iteration:  970  Classification rate:  0.1296605453533667  Cost:  5087.703402171699\n",
      "Iteration:  971  Classification rate:  0.1296605453533667  Cost:  5086.9046472327545\n",
      "Iteration:  972  Classification rate:  0.1296605453533667  Cost:  5086.106453678428\n",
      "Iteration:  973  Classification rate:  0.1296605453533667  Cost:  5085.30882070435\n",
      "Iteration:  974  Classification rate:  0.1296605453533667  Cost:  5084.511747507945\n",
      "Iteration:  975  Classification rate:  0.1296605453533667  Cost:  5083.715233288424\n",
      "Iteration:  976  Classification rate:  0.12910406232609906  Cost:  5082.919277246782\n",
      "Iteration:  977  Classification rate:  0.12910406232609906  Cost:  5082.123878585799\n",
      "Iteration:  978  Classification rate:  0.12910406232609906  Cost:  5081.32903651003\n",
      "Iteration:  979  Classification rate:  0.12910406232609906  Cost:  5080.534750225808\n",
      "Iteration:  980  Classification rate:  0.12910406232609906  Cost:  5079.741018941231\n",
      "Iteration:  981  Classification rate:  0.12910406232609906  Cost:  5078.947841866166\n",
      "Iteration:  982  Classification rate:  0.12910406232609906  Cost:  5078.155218212241\n",
      "Iteration:  983  Classification rate:  0.12910406232609906  Cost:  5077.363147192849\n",
      "Iteration:  984  Classification rate:  0.1296605453533667  Cost:  5076.571628023131\n",
      "Iteration:  985  Classification rate:  0.1296605453533667  Cost:  5075.780659919983\n",
      "Iteration:  986  Classification rate:  0.1296605453533667  Cost:  5074.990242102046\n",
      "Iteration:  987  Classification rate:  0.1302170283806344  Cost:  5074.200373789712\n",
      "Iteration:  988  Classification rate:  0.1302170283806344  Cost:  5073.411054205107\n",
      "Iteration:  989  Classification rate:  0.13077351140790205  Cost:  5072.622282572092\n",
      "Iteration:  990  Classification rate:  0.1302170283806344  Cost:  5071.834058116272\n",
      "Iteration:  991  Classification rate:  0.1302170283806344  Cost:  5071.046380064969\n",
      "Iteration:  992  Classification rate:  0.1302170283806344  Cost:  5070.259247647239\n",
      "Iteration:  993  Classification rate:  0.1302170283806344  Cost:  5069.472660093855\n",
      "Iteration:  994  Classification rate:  0.1302170283806344  Cost:  5068.686616637313\n",
      "Iteration:  995  Classification rate:  0.1296605453533667  Cost:  5067.901116511821\n",
      "Iteration:  996  Classification rate:  0.1302170283806344  Cost:  5067.116158953298\n",
      "Iteration:  997  Classification rate:  0.1302170283806344  Cost:  5066.331743199372\n",
      "Iteration:  998  Classification rate:  0.1302170283806344  Cost:  5065.547868489375\n",
      "Iteration:  999  Classification rate:  0.1302170283806344  Cost:  5064.764534064338\n"
     ]
    }
   ],
   "source": [
    "N.gradient_descent(1000, 0.001, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
