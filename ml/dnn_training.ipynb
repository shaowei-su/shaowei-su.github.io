{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up prev model path: tfmodel\n",
      "Input Part File:  214092831 => train.csv\n",
      "Total Train Input Bytes: 214092831\n",
      "Total Train Input Records: 1427285\n",
      "Using Train Batch Size: 1000\n",
      "Using Train Epochs: 1\n",
      "Using Train Steps Per Epoch: 1427\n",
      "Using Train Steps Global: 1427\n",
      "Using Test Steps Global: 10\n",
      "Using SparkMode: 0\n",
      "Using FLAGS: Namespace(app_name='HappiCFDnn', clean_model=1, dropout=0.0, h1=16, h2=16, h3=8, h4=8, l1=0.0, l2=1e-06, learning_rate=0.001, model_dict='dict.tsv', model_dir='tfmodel', model_type='deep', num_ps=1, optimizer='adagrad', record_count_tsv='counter.tsv', spark_mode=0, start_ts=1555890801501, summary='summary.txt', task_index=0, test_batch_size=5000, test_data='test.csv', test_steps=10, total_workers=1, train_batch_size=1000, train_data='train.csv', train_epochs=1, train_records=0, train_steps=1427)\n",
      "Reading Dictionary: dict.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-21 16:53:30,163 INFO (MainThread-97467) Using config: {'_model_dir': 'tfmodel/tf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0xb1f8acf98>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "2019-04-21 16:53:30,165 INFO (MainThread-97467) Running training and evaluation locally (non-distributed).\n",
      "2019-04-21 16:53:30,165 INFO (MainThread-97467) Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "2019-04-21 16:53:30,225 INFO (MainThread-97467) Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fdict: {'ad_adg': 65744, 'ad_adv': 8296, 'ad_cmp': 49681, 'ad_id': 105469, 'ad_sponsored_by': 7504, 'ad_title': 27931, 'country_woeid': 191, 'page_section_id': 85, 'state_woeid': 1472, 'user': 2785208, 'campaign_objective': 9, 'adv_sub_category': 134, 'dayofweek': 7, 'daypart': 4, 'user_age': 11, 'user_gender': 3, 'user_os': 6, 'device_type': 2}\n",
      "CSV_FEATURE_NAMES: ['event_guid', 'click', 'campaign_objective', 'adv_sub_category', 'ad_id', 'ad_cmp', 'ad_adv', 'ad_adg', 'ad_sponsored_by', 'ad_title', 'dayofweek', 'daypart', 'user', 'user_age', 'user_gender', 'user_os', 'device_type', 'state_woeid', 'country_woeid', 'page_section_id']\n",
      "NAME: ad_adg               TYPE: EMBEDDING\t SIZE: 65744\t DIMENSION: 17\n",
      "NAME: ad_adv               TYPE: EMBEDDING\t SIZE: 8296\t DIMENSION: 14\n",
      "NAME: ad_cmp               TYPE: EMBEDDING\t SIZE: 49681\t DIMENSION: 16\n",
      "NAME: ad_id                TYPE: EMBEDDING\t SIZE: 105469\t DIMENSION: 17\n",
      "NAME: ad_sponsored_by      TYPE: EMBEDDING\t SIZE: 7504\t DIMENSION: 13\n",
      "NAME: ad_title             TYPE: EMBEDDING\t SIZE: 27931\t DIMENSION: 15\n",
      "NAME: adv_sub_category     TYPE: INDICATOR\t SIZE: 134\n",
      "NAME: campaign_objective   TYPE: INDICATOR\t SIZE: 9\n",
      "NAME: country_woeid        TYPE: INDICATOR\t SIZE: 191\n",
      "NAME: dayofweek            TYPE: INDICATOR\t SIZE: 7\n",
      "NAME: daypart              TYPE: INDICATOR\t SIZE: 4\n",
      "NAME: device_type          TYPE: INDICATOR\t SIZE: 2\n",
      "NAME: page_section_id      TYPE: INDICATOR\t SIZE: 85\n",
      "NAME: state_woeid          TYPE: EMBEDDING\t SIZE: 1472\t DIMENSION: 11\n",
      "NAME: user                 TYPE: EMBEDDING\t SIZE: 2785208\t DIMENSION: 32\n",
      "NAME: user_age             TYPE: INDICATOR\t SIZE: 11\n",
      "NAME: user_gender          TYPE: INDICATOR\t SIZE: 3\n",
      "NAME: user_os              TYPE: INDICATOR\t SIZE: 6\n",
      "Using DNNClassifier with Hidden Units: [16, 16, 8, 8]\n",
      "Using Model Dir: tfmodel/tf\n",
      "Using Learning Rate: 0.001000\n",
      "Using L1 Regularization Strength: 0.000000\n",
      "Using L2 Regularization Strength: 0.000001\n",
      "Using ModelType: deep\n",
      "Using Dropout: None\n",
      "Input Function => Using data_file: train.csv\n",
      "Input Function => Using num_epochs: 1\n",
      "Input Function => Using task_index: 0\n",
      "Input Function => Using total_workers: 1\n",
      "Input Part File: train.csv\n",
      "Total Workers: 1; Total Input Files: 1\n",
      "Worker Index: 0; Total Workers: 1; Adding File (000): train.csv\n",
      "Worker Index: 0; Total Workers: 1; Total Files to process: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-21 16:53:31,671 INFO (MainThread-97467) Done calling model_fn.\n",
      "2019-04-21 16:53:31,673 INFO (MainThread-97467) Create CheckpointSaverHook.\n",
      "2019-04-21 16:53:32,206 INFO (MainThread-97467) Graph was finalized.\n",
      "2019-04-21 16:53:35,218 INFO (MainThread-97467) Running local_init_op.\n",
      "2019-04-21 16:53:35,242 INFO (MainThread-97467) Done running local_init_op.\n",
      "2019-04-21 16:53:36,206 INFO (MainThread-97467) Saving checkpoints for 0 into tfmodel/tf/model.ckpt.\n",
      "2019-04-21 16:53:42,688 INFO (MainThread-97467) loss = 609.2002, step = 1\n",
      "2019-04-21 16:53:45,294 INFO (MainThread-97467) global_step/sec: 38.3594\n",
      "2019-04-21 16:53:45,296 INFO (MainThread-97467) loss = 284.28558, step = 101 (2.608 sec)\n",
      "2019-04-21 16:53:47,043 INFO (MainThread-97467) global_step/sec: 57.1931\n",
      "2019-04-21 16:53:47,044 INFO (MainThread-97467) loss = 162.46823, step = 201 (1.748 sec)\n",
      "2019-04-21 16:53:48,776 INFO (MainThread-97467) global_step/sec: 57.6723\n",
      "2019-04-21 16:53:48,778 INFO (MainThread-97467) loss = 109.745834, step = 301 (1.734 sec)\n",
      "2019-04-21 16:53:50,468 INFO (MainThread-97467) global_step/sec: 59.1003\n",
      "2019-04-21 16:53:50,470 INFO (MainThread-97467) loss = 77.17489, step = 401 (1.692 sec)\n",
      "2019-04-21 16:53:52,205 INFO (MainThread-97467) global_step/sec: 57.5915\n",
      "2019-04-21 16:53:52,206 INFO (MainThread-97467) loss = 78.39064, step = 501 (1.736 sec)\n",
      "2019-04-21 16:53:53,910 INFO (MainThread-97467) global_step/sec: 58.6438\n",
      "2019-04-21 16:53:53,911 INFO (MainThread-97467) loss = 48.6762, step = 601 (1.705 sec)\n",
      "2019-04-21 16:53:55,623 INFO (MainThread-97467) global_step/sec: 58.3633\n",
      "2019-04-21 16:53:55,625 INFO (MainThread-97467) loss = 41.40796, step = 701 (1.714 sec)\n",
      "2019-04-21 16:53:57,305 INFO (MainThread-97467) global_step/sec: 59.4829\n",
      "2019-04-21 16:53:57,306 INFO (MainThread-97467) loss = 40.73675, step = 801 (1.682 sec)\n",
      "2019-04-21 16:53:58,985 INFO (MainThread-97467) global_step/sec: 59.5087\n",
      "2019-04-21 16:53:58,987 INFO (MainThread-97467) loss = 35.708935, step = 901 (1.680 sec)\n",
      "2019-04-21 16:54:00,661 INFO (MainThread-97467) Saving checkpoints for 1000 into tfmodel/tf/model.ckpt.\n",
      "2019-04-21 16:54:05,971 INFO (MainThread-97467) Loss for final step: 45.609642.\n",
      "2019-04-21 16:54:06,011 INFO (MainThread-97467) Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Function => Using data_file: test.csv\n",
      "Input Function => Using num_epochs: 1\n",
      "Input Function => Using task_index: 0\n",
      "Input Function => Using total_workers: 1\n",
      "Input Part File: test.csv\n",
      "Total Workers: 1; Total Input Files: 1\n",
      "Worker Index: 0; Total Workers: 1; Adding File (000): test.csv\n",
      "Worker Index: 0; Total Workers: 1; Total Files to process: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-21 16:54:08,101 WARNING (MainThread-97467) Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "2019-04-21 16:54:08,129 WARNING (MainThread-97467) Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "2019-04-21 16:54:08,153 INFO (MainThread-97467) Done calling model_fn.\n",
      "2019-04-21 16:54:08,172 INFO (MainThread-97467) Starting evaluation at 2019-04-21-23:54:08\n",
      "2019-04-21 16:54:08,270 INFO (MainThread-97467) Graph was finalized.\n",
      "2019-04-21 16:54:08,271 INFO (MainThread-97467) Restoring parameters from tfmodel/tf/model.ckpt-1000\n",
      "2019-04-21 16:54:09,134 INFO (MainThread-97467) Running local_init_op.\n",
      "2019-04-21 16:54:09,174 INFO (MainThread-97467) Done running local_init_op.\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "test.csv; No such file or directory\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n\nCaused by op 'IteratorGetNext', defined at:\n  File \"/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-628db2604f4e>\", line 491, in <module>\n    main_local()\n  File \"<ipython-input-1-628db2604f4e>\", line 421, in main_local\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 447, in train_and_evaluate\n    return executor.run()\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 531, in run\n    return self.run_local()\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 681, in run_local\n    eval_result, export_results = evaluator.evaluate_and_export()\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 886, in evaluate_and_export\n    hooks=self._eval_spec.hooks)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 453, in evaluate\n    input_fn, hooks, checkpoint_path)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1346, in _evaluate_build_graph\n    model_fn_lib.ModeKeys.EVAL))\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 985, in _get_features_and_labels_from_input_fn\n    result = self._call_input_fn(input_fn, mode)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1074, in _call_input_fn\n    return input_fn(**kwargs)\n  File \"<ipython-input-1-628db2604f4e>\", line 283, in input_fn\n    features, labels = parse_input(iterator.get_next())\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 373, in get_next\n    name=name)), self._output_types,\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1745, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): test.csv; No such file or directory\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: test.csv; No such file or directory\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-628db2604f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mtf_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mmain_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-628db2604f4e>\u001b[0m in \u001b[0;36mmain_local\u001b[0;34m()\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0mtrain_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0meval_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvalSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;31m# Export_Saved Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[1;32m    445\u001b[0m         '(with task id 0).  Given task id {}'.format(config.task_id))\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    529\u001b[0m         config.task_type != run_config_lib.TaskType.EVALUATOR):\n\u001b[1;32m    530\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running training and evaluation locally (non-distributed).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;31m# Distributed case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mrun_local\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m       \u001b[0;31m# condition is satisfied (both checks use the same global_step value,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0;31m# i.e., no race condition)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m       \u001b[0meval_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_and_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0meval_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_EvalStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEVALUATED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mevaluate_and_export\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    884\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatest_ckpt_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m           hooks=self._eval_spec.hooks)\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;31m# _EvalResult validates the metrics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0meval_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mall_hooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             output_dir=self.eval_dir(name))\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_convert_eval_steps_to_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate_run\u001b[0;34m(self, checkpoint_path, scaffold, update_op, eval_dict, all_hooks, output_dir)\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0mfinal_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m         config=self._session_config)\n\u001b[0m\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m     \u001b[0mcurrent_global_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_STEP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/evaluation.py\u001b[0m in \u001b[0;36m_evaluate_once\u001b[0;34m(checkpoint_path, master, scaffold, eval_ops, feed_dict, final_ops, final_ops_feed_dict, hooks, config)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meval_ops\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m   logging.info('Finished evaluation at ' + time.strftime('%Y-%m-%d-%H:%M:%S',\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    575\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1051\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1054\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1199\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: test.csv; No such file or directory\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n\nCaused by op 'IteratorGetNext', defined at:\n  File \"/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-628db2604f4e>\", line 491, in <module>\n    main_local()\n  File \"<ipython-input-1-628db2604f4e>\", line 421, in main_local\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 447, in train_and_evaluate\n    return executor.run()\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 531, in run\n    return self.run_local()\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 681, in run_local\n    eval_result, export_results = evaluator.evaluate_and_export()\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 886, in evaluate_and_export\n    hooks=self._eval_spec.hooks)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 453, in evaluate\n    input_fn, hooks, checkpoint_path)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1346, in _evaluate_build_graph\n    model_fn_lib.ModeKeys.EVAL))\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 985, in _get_features_and_labels_from_input_fn\n    result = self._call_input_fn(input_fn, mode)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1074, in _call_input_fn\n    return input_fn(**kwargs)\n  File \"<ipython-input-1-628db2604f4e>\", line 283, in input_fn\n    features, labels = parse_input(iterator.get_next())\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 373, in get_next\n    name=name)), self._output_types,\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1745, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/Users/shsu33/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): test.csv; No such file or directory\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from tensorflowonspark import TFCluster\n",
    "\n",
    "import argparse\n",
    "import shutil\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "#import logging\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "#FORMAT = '[%(asctime)-15s] [%(levelname)s] - %(message)s'\n",
    "#logging.basicConfig(level=logging.INFO, format=FORMAT)\n",
    "#logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_arg_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "    \n",
    "    parser.add_argument('--spark_mode', type=int, default=0, help='Run in Spark Mode')\n",
    "    parser.add_argument('--clean_model', type=int, default=1, help='Clean Up Previous Existing Model Dir')\n",
    "    parser.add_argument('--model_dir', type=str, default='tfmodel', help='Base directory for the model.')\n",
    "    parser.add_argument('--model_dict', type=str, default='dict.tsv', help='Feature Count')\n",
    "    \n",
    "    parser.add_argument('--train_batch_size', type=int, default=1000, help='Number of examples per batch for training')\n",
    "    parser.add_argument('--train_records', type=int, default=0, help='Total Number of Records; If 0 then determine based on input FileSize')\n",
    "    parser.add_argument('--train_steps', type=int, default=0, help='Number of training steps; If 0 then determine based on input FileSize')\n",
    "    parser.add_argument('--train_epochs', type=int, default=1, help='Number of training epochs.')\n",
    "\n",
    "    parser.add_argument('--test_batch_size', type=int, default=5000, help='Number of examples per batch for testing')\n",
    "    parser.add_argument('--test_steps', type=int, default=10, help='Number of testing steps')\n",
    "\n",
    "    parser.add_argument('--record_count_tsv', type=str, default='counter.tsv', help='Path to the training data.')\n",
    "    parser.add_argument('--train_data', type=str, default='train.csv', help='Path to the training data.')\n",
    "    parser.add_argument('--test_data', type=str, default='test.csv', help='Path to the test data.')\n",
    "    \n",
    "    parser.add_argument('--model_type', type=str, default='deep', help='deep, wide, wide_deep')\n",
    "    parser.add_argument('--optimizer', type=str, default='adagrad', help='Optimizer (adagrad, adam)')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning Rate')\n",
    "    parser.add_argument('--l1', type=float, default=0.0, help='l1 regularization strength')\n",
    "    parser.add_argument('--l2', type=float, default=0.000001, help='l2 regularization strength')\n",
    "    parser.add_argument('--dropout', type=float, default=0.0, help='Drop Out Probability between 0 to 1')\n",
    "\n",
    "    parser.add_argument('--h1', type=int, default=16, help='Number of Hidden Units - Layer1')\n",
    "    parser.add_argument('--h2', type=int, default=16, help='Number of Hidden Units - Layer2')\n",
    "    parser.add_argument('--h3', type=int, default=8, help='Number of Hidden Units - Layer3')\n",
    "    parser.add_argument('--h4', type=int, default=8, help='Number of Hidden Units - Layer4')\n",
    "    \n",
    "    parser.add_argument('--num_ps', type=int, default=1, help='Number of PS Servers')\n",
    "    parser.add_argument('--app_name', type=str, default=\"HappiCFDnn\", help='Name of Spark Application')\n",
    "    parser.add_argument('--summary', type=str, default=\"summary.txt\", help='Summary of the experiment and results')\n",
    "    return parser\n",
    "\n",
    "CSV_FEATURE_NAMES = []\n",
    "CSV_FEATURE_DEFAULTS = []\n",
    "\n",
    "################################################################################################################\n",
    "class FeatureColumType:\n",
    "    def __init__(self, coltype, name, vtype, size=0, dimension=0, max_bucket=0):\n",
    "        self.coltype = coltype\n",
    "        self.name = name\n",
    "        self.vtype = vtype\n",
    "        self.size = size\n",
    "        self.dimension = dimension\n",
    "        self.max_bucket = max_bucket\n",
    "        \n",
    "feature_columns = []\n",
    "feature_columns_info = []\n",
    "fdict = {}\n",
    "\n",
    "def fetch_column_info(model_dict):\n",
    "    global CSV_FEATURE_NAMES\n",
    "    global CSV_FEATURE_DEFAULTS\n",
    "    global fdict\n",
    "\n",
    "    CSV_ALL_COLS = [\n",
    "        'event_guid', 'click',\n",
    "        'campaign_objective', 'adv_sub_category',\n",
    "        'ad_id', 'ad_cmp', 'ad_adv', 'ad_adg',\n",
    "        'ad_sponsored_by', 'ad_title',\n",
    "        'dayofweek', 'daypart',\n",
    "        'user', 'user_age', 'user_gender', 'user_os', 'device_type',\n",
    "        'state_woeid', 'country_woeid',\n",
    "        'page_section_id']\n",
    "\n",
    "    print('Reading Dictionary: %s' % model_dict)\n",
    "    tfdict = tf.gfile.GFile(model_dict)\n",
    "\n",
    "    for line in tfdict.readlines():\n",
    "        fields = line.strip().split('\\t')\n",
    "        name = fields[1]\n",
    "        if name in fdict:\n",
    "            fdict[name] = fdict[name] + 1\n",
    "        else:\n",
    "            fdict[name] = 1\n",
    "    print('fdict: %s' % fdict)\n",
    "\n",
    "    for f in CSV_ALL_COLS:\n",
    "        if f == 'event_guid' or f == 'click' or f in fdict:\n",
    "            CSV_FEATURE_NAMES.append(f)\n",
    "            CSV_FEATURE_DEFAULTS.append(['0'] if (f == 'event_guid' or f == 'ad_title') else [0])\n",
    "    print('CSV_FEATURE_NAMES: %s' % CSV_FEATURE_NAMES)\n",
    "\n",
    "def normalize_column(col):\n",
    "    return (col - np.mean(col)) / np.std(col)\n",
    "\n",
    "def fdim(total):\n",
    "    return int(np.log2(total))+1\n",
    "\n",
    "def build_wide_model_columns():\n",
    "    global fdict\n",
    "    global feature_columns\n",
    "    global feature_columns_info\n",
    "    ####################################################################\n",
    "    for name in sorted(fdict.keys()):\n",
    "        size = fdict[name]\n",
    "        if size < 256:\n",
    "            feature_columns.append(tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity(key=name, num_buckets=size)))\n",
    "            feature_columns_info.append(FeatureColumType(coltype='INDICATOR', name=name, vtype='int32', size=size, dimension=0))\n",
    "            print('NAME: %-20s TYPE: INDICATOR\\t SIZE: %d' % (name, size))\n",
    "        else:\n",
    "            dimension = 32 if name == 'user' else fdim(size)\n",
    "            feature_columns.append(tf.feature_column.embedding_column(tf.feature_column.categorical_column_with_identity(key=name, num_buckets=size), dimension))\n",
    "            feature_columns_info.append(FeatureColumType(coltype='EMBEDDING', name=name, vtype='int32', size=size, dimension=dimension))\n",
    "            print('NAME: %-20s TYPE: EMBEDDING\\t SIZE: %d\\t DIMENSION: %d' % (name, size, dimension))\n",
    "    ####################################################################\n",
    "\n",
    "    return feature_columns, feature_columns\n",
    "\n",
    "\n",
    "def build_estimator(model_dir, config):\n",
    "    deep_columns, wide_columns = build_wide_model_columns()\n",
    "\n",
    "    hidden_units = []\n",
    "    hidden_units.append(FLAGS.h1)\n",
    "    if FLAGS.h2 > 0:\n",
    "        hidden_units.append(FLAGS.h2)\n",
    "    if FLAGS.h3 > 0:\n",
    "        hidden_units.append(FLAGS.h3)\n",
    "    if FLAGS.h4 > 0:\n",
    "        hidden_units.append(FLAGS.h4)\n",
    "\n",
    "    dropout = None\n",
    "    if FLAGS.dropout > 0.0:\n",
    "        dropout = FLAGS.dropout\n",
    "    \n",
    "    print('Using DNNClassifier with Hidden Units: %s' % hidden_units)\n",
    "    print('Using Model Dir: %s' % model_dir)\n",
    "    print('Using Learning Rate: %f' % FLAGS.learning_rate)\n",
    "    print('Using L1 Regularization Strength: %f' % FLAGS.l1)\n",
    "    print('Using L2 Regularization Strength: %f' % FLAGS.l2)\n",
    "    print('Using ModelType: %s' % FLAGS.model_type)\n",
    "    print('Using Dropout: %s' % dropout)\n",
    "    \n",
    "    optimzer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate) if FLAGS.optimizer == 'adam' else tf.train.ProximalAdagradOptimizer(learning_rate=FLAGS.learning_rate, l1_regularization_strength=FLAGS.l1, l2_regularization_strength=FLAGS.l2)\n",
    "    \n",
    "    if FLAGS.model_type == \"wide\":\n",
    "        return tf.estimator.LinearClassifier(\n",
    "        model_dir=model_dir,\n",
    "        feature_columns=wide_columns,\n",
    "        config=config)\n",
    "\n",
    "    if FLAGS.model_type == \"wide_deep\":\n",
    "        return tf.estimator.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_dir,\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=hidden_units,\n",
    "        config=config,\n",
    "        dnn_dropout=dropout,\n",
    "        dnn_optimizer=optimzer)\n",
    "\n",
    "    return tf.estimator.DNNClassifier(\n",
    "        model_dir=model_dir,\n",
    "        feature_columns=deep_columns,\n",
    "        hidden_units=hidden_units,\n",
    "        dropout=dropout,\n",
    "        config=config,\n",
    "        optimizer=optimzer)\n",
    "\n",
    "def parse_multival(x):\n",
    "    sparse_strings = tf.string_split(x, delimiter=\"\\x02\")\n",
    "    return tf.SparseTensor(indices=sparse_strings.indices, values=tf.string_to_number(sparse_strings.values, out_type=tf.int32), dense_shape=sparse_strings.dense_shape)\n",
    "\n",
    "def parse_input(value):\n",
    "    columns = tf.decode_csv(value, record_defaults=CSV_FEATURE_DEFAULTS)\n",
    "    features = dict(zip(CSV_FEATURE_NAMES, columns))\n",
    "\n",
    "    if 'ad_title' in features:\n",
    "        features['ad_title'] = parse_multival(features['ad_title'])\n",
    "\n",
    "    features.pop('event_guid')\n",
    "    label = features.pop('click')\n",
    "    return features, label\n",
    "\n",
    "def export_input_fn():\n",
    "    csv_row = tf.placeholder(dtype=tf.string, shape=[None], name='input_csv_tensor')\n",
    "    receiver_tensors = {'csv_row': csv_row}\n",
    "    \n",
    "    columns = tf.decode_csv(csv_row, record_defaults=CSV_FEATURE_DEFAULTS)\n",
    "    features = dict(zip(CSV_FEATURE_NAMES, columns))\n",
    "\n",
    "    if 'ad_title' in features:\n",
    "        features['ad_title'] = parse_multival(features['ad_title'])\n",
    "\n",
    "    features.pop('event_guid')\n",
    "    features.pop('click')\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n",
    "\n",
    "def get_all_input_files(data_file):\n",
    "    input_files = []\n",
    "    if tf.gfile.IsDirectory(data_file):\n",
    "        filenames = tf.gfile.ListDirectory(data_file)\n",
    "        for f in filenames:\n",
    "            if str(f).startswith('part'):\n",
    "                infile = data_file + '/' + f\n",
    "                input_files.append(infile)\n",
    "                print('Input Part File: %s' % infile)\n",
    "    else:\n",
    "        input_files = [data_file]\n",
    "        print('Input Part File: %s' % data_file)\n",
    "    return input_files\n",
    "\n",
    "def get_all_input_size(data_file):\n",
    "    total_input_size = 0\n",
    "    if tf.gfile.IsDirectory(data_file):\n",
    "        filenames = tf.gfile.ListDirectory(data_file)\n",
    "        for f in filenames:\n",
    "            if str(f).startswith('part'):\n",
    "                infile = data_file + '/' + f\n",
    "                fstats = tf.gfile.Stat(infile)\n",
    "                total_input_size += fstats.length\n",
    "                print('Input Part File Size: %10d => %s' % (fstats.length, infile))\n",
    "    else:\n",
    "        fstats = tf.gfile.Stat(data_file)\n",
    "        total_input_size = fstats.length\n",
    "        print('Input Part File: %10d => %s' % (fstats.length, data_file))\n",
    "    return total_input_size\n",
    "\n",
    "\n",
    "def input_fn(data_file, num_epochs, batch_size, task_index, total_workers):\n",
    "    print('Input Function => Using data_file: %s' % (data_file))\n",
    "    print('Input Function => Using num_epochs: %d' % (num_epochs))\n",
    "    print('Input Function => Using task_index: %d' % (task_index))\n",
    "    print('Input Function => Using total_workers: %d' % (total_workers))\n",
    "    \n",
    "    part_files = get_all_input_files(data_file)\n",
    "    print('Total Workers: %d; Total Input Files: %d' % (total_workers, len(part_files)))\n",
    "    \n",
    "    input_files = []\n",
    "    for epoch_index in range(0, num_epochs):\n",
    "        tindex = (task_index + epoch_index) % total_workers\n",
    "        \n",
    "        file_id = 0\n",
    "        for infile in part_files:\n",
    "            if file_id % total_workers == tindex:\n",
    "                input_files.append(infile)\n",
    "                print('Worker Index: %d; Total Workers: %d; Adding File (%03d): %s' % (task_index, total_workers, file_id, infile))\n",
    "            file_id = file_id + 1\n",
    "    \n",
    "    print('Worker Index: %d; Total Workers: %d; Total Files to process: %d' % (task_index, total_workers, len(input_files)))\n",
    "\n",
    "    dataset = tf.data.TextLineDataset(input_files)\n",
    "    if total_workers == 1:\n",
    "        dataset = dataset.repeat(1)\n",
    "    else:\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    #dataset = dataset.map(parse_input)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    #features, labels = iterator.get_next()\n",
    "    features, labels = parse_input(iterator.get_next())\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def log_flags_info(out_file):\n",
    "    for arg in vars(FLAGS):\n",
    "        line = 'FLAGS\\t' + arg + '\\t' + str(getattr(FLAGS, arg))\n",
    "        out_file.write(line + '\\n')\n",
    "        print(line)\n",
    "    \n",
    "def log_traing_info(out_file):\n",
    "    steps_avg_loss = []\n",
    "    eventfile = \"\"\n",
    "    for f in tf.gfile.ListDirectory(FLAGS.model_dir + \"/tf\"):\n",
    "        if f.startswith('events'):\n",
    "            eventfile = FLAGS.model_dir + \"/tf/\" + f\n",
    "    for summary in tf.train.summary_iterator(eventfile):\n",
    "        for v in summary.summary.value:\n",
    "            if v.tag == 'average_loss':\n",
    "                steps_avg_loss.append(v.simple_value)\n",
    "    \n",
    "    steps_per_iteration = int(FLAGS.train_steps/FLAGS.train_epochs)\n",
    "    line = 'TRAIN\\tStepsPerIteration\\t' + str(steps_per_iteration)\n",
    "    out_file.write(line + '\\n')\n",
    "    print(line)\n",
    "\n",
    "    step_log_count = int((len(steps_avg_loss) + FLAGS.train_epochs - 1) / FLAGS.train_epochs)\n",
    "    for i in range(0, FLAGS.train_epochs):\n",
    "        startRange = i * step_log_count\n",
    "        endRange = min(startRange + step_log_count, len(steps_avg_loss))\n",
    "        line = 'ITERATION\\t' + ('%03d' % (i)) + '\\tAverageLoss\\t' + str(scipy.mean(steps_avg_loss[startRange:endRange]))\n",
    "        out_file.write(line + '\\n')\n",
    "        print(line)\n",
    "        \n",
    "    line = 'TRAIN\\tFinalAverageLoss\\t' + str(scipy.mean(steps_avg_loss))\n",
    "    out_file.write(line + '\\n')\n",
    "    print(line)\n",
    "\n",
    "    line = 'TRAIN\\tTrainPeriod\\t' + str(int(round(time.time() * 1000)) - FLAGS.start_ts)\n",
    "    out_file.write(line + '\\n')\n",
    "    print(line)\n",
    "\n",
    "def log_feature_info(out_file):\n",
    "    global feature_columns_info\n",
    "    layer1_feature_count = 0\n",
    "    for info in feature_columns_info:\n",
    "        if info.coltype == \"EMBEDDING\":\n",
    "            layer1_feature_count += info.dimension\n",
    "        else:\n",
    "            layer1_feature_count += info.size\n",
    "        line = 'FEATURE\\t' + info.coltype + '\\t' + info.name + '\\t' + info.vtype + '\\t' + str(info.size) + '\\t' + str(info.dimension) + '\\t' + str(info.max_bucket)\n",
    "        out_file.write(line + '\\n')\n",
    "        print(line)\n",
    "    \n",
    "    line = 'TRAIN\\tLayer1FeatureCount\\t' + str(layer1_feature_count)\n",
    "    out_file.write(line + '\\n')\n",
    "    print(line)\n",
    "\n",
    "def log_summary():\n",
    "    print(\"+++++++++++++++++++++++++++++ LOGGING SUMMARY +++++++++++++++++++++++++++++\")\n",
    "    out_file = tf.gfile.GFile(FLAGS.summary, mode='w')\n",
    "    \n",
    "    log_flags_info(out_file)\n",
    "    log_traing_info(out_file)\n",
    "    log_feature_info(out_file)\n",
    "\n",
    "    out_file.close()\n",
    "\n",
    "def main_spark(unused_argv, ctx):\n",
    "    cluster_spec = ctx.cluster_spec\n",
    "    job_name = ctx.job_name\n",
    "    task_index = ctx.task_index\n",
    "\n",
    "    # Keep the original ones for distributing input files among workers\n",
    "    FLAGS.task_index = task_index\n",
    "    FLAGS.total_workers = len(cluster_spec[\"worker\"])\n",
    "\n",
    "    cluster_spec[\"master\"] = [cluster_spec[\"worker\"][0]]\n",
    "    cluster_spec[\"worker\"] = cluster_spec[\"worker\"][1:]\n",
    "    \n",
    "    if task_index == 0 and job_name == \"worker\":\n",
    "        job_name = \"master\"\n",
    "    elif job_name == \"worker\":\n",
    "        task_index = ctx.task_index - 1\n",
    "    \n",
    "    print('Current Node - Job Name: %s; Task Index: %s' % (job_name, task_index))\n",
    "    print('Cluster specification is: %s' % cluster_spec)\n",
    "    print('Cluster specification for Master is: %s' % cluster_spec[\"master\"])\n",
    "\n",
    "    print('Using FLAGS: %s' % (FLAGS))\n",
    "    fetch_column_info(FLAGS.model_dict)\n",
    "\n",
    "    if job_name == \"ps\":\n",
    "        os.environ['TF_CONFIG'] = json.dumps({'cluster': cluster_spec, 'environment': 'cloud', 'task': {'type': 'ps', 'index': task_index}})\n",
    "    elif job_name == \"master\":\n",
    "        os.environ['TF_CONFIG'] = json.dumps({'cluster': cluster_spec, 'environment': 'cloud', 'task': {'type': 'master', 'index': task_index}})\n",
    "    else:\n",
    "        os.environ['TF_CONFIG'] = json.dumps({'cluster': cluster_spec, 'environment': 'cloud', 'task': {'type': 'worker', 'index': task_index}})\n",
    "    \n",
    "    print('Using TFConfig: %s' % os.environ['TF_CONFIG'])\n",
    "    \n",
    "    output_dir = FLAGS.model_dir + \"/tf\"\n",
    "\n",
    "    config = tf.estimator.RunConfig(model_dir=output_dir)\n",
    "    train_input_fn = functools.partial(input_fn, FLAGS.train_data, FLAGS.train_epochs, FLAGS.train_batch_size, FLAGS.task_index, FLAGS.total_workers)\n",
    "    eval_input_fn = functools.partial(input_fn, FLAGS.test_data, 1, FLAGS.test_batch_size, 0, 1)\n",
    "    estimator = build_estimator(output_dir, config)\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=FLAGS.train_steps)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, steps=FLAGS.test_steps)\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "    # Export_Saved Model\n",
    "    if job_name == \"master\":\n",
    "        print(\"Trying to save the model for serving through master node\")\n",
    "        servable_model_path = estimator.export_savedmodel(FLAGS.model_dir + \"/serve\", export_input_fn, as_text=False)\n",
    "        print(\"++++++ Servable Model Path: %s\" % servable_model_path)\n",
    "        \n",
    "        # Save Summary of this experiment\n",
    "        log_summary()\n",
    "\n",
    "\n",
    "def main_local():\n",
    "    FLAGS.task_index = 0\n",
    "    FLAGS.total_workers = 1\n",
    "    print('Using FLAGS: %s' % (FLAGS))\n",
    "    \n",
    "    fetch_column_info(FLAGS.model_dict)\n",
    "\n",
    "    output_dir = FLAGS.model_dir + \"/tf\"\n",
    "    \n",
    "    config = tf.estimator.RunConfig(model_dir=output_dir)\n",
    "    train_input_fn = functools.partial(input_fn, FLAGS.train_data, FLAGS.train_epochs, FLAGS.train_batch_size, FLAGS.task_index, FLAGS.total_workers)\n",
    "    eval_input_fn = functools.partial(input_fn, FLAGS.test_data, 1, FLAGS.test_batch_size, 0, 1)\n",
    "    estimator = build_estimator(output_dir, config)\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=FLAGS.train_steps)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, steps=FLAGS.test_steps)\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "    # Export_Saved Model\n",
    "    servable_model_path = estimator.export_savedmodel(FLAGS.model_dir + \"/serve\", export_input_fn, as_text=True)\n",
    "    print('++++++ Servable Model Path: %s' % servable_model_path)\n",
    "\n",
    "    # Save Summary of this experiment\n",
    "    log_summary()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    FLAGS, unparsed = get_arg_parser().parse_known_args()\n",
    "\n",
    "    if FLAGS.clean_model == 1:\n",
    "        print('Cleaning up prev model path: %s' % (FLAGS.model_dir))\n",
    "        shutil.rmtree(FLAGS.model_dir, ignore_errors=True)\n",
    "    FLAGS.start_ts = int(round(time.time() * 1000))\n",
    "\n",
    "    total_input_size = 0\n",
    "    total_input_records = 0\n",
    "    total_test_records = 0\n",
    "    total_input_steps_per_epoch = 0\n",
    "    total_input_steps_global = 0\n",
    "    if FLAGS.train_records == 0 and FLAGS.train_steps == 0:\n",
    "        if tf.gfile.Exists(FLAGS.record_count_tsv):\n",
    "            counterTsv = tf.gfile.GFile(FLAGS.record_count_tsv)\n",
    "            for line in counterTsv.readlines():\n",
    "                fields = line.strip().split('\\t')\n",
    "                if fields[0] == 'train':\n",
    "                    total_input_records = int(fields[1])\n",
    "                if fields[0] == 'test':\n",
    "                    total_test_records = int(fields[1])\n",
    "        else:\n",
    "            total_input_size = get_all_input_size(FLAGS.train_data)\n",
    "            total_input_records = int(total_input_size / 150)\n",
    "\n",
    "    if FLAGS.train_steps == 0:\n",
    "        total_input_steps_per_epoch = int(total_input_records / FLAGS.train_batch_size)\n",
    "        total_input_steps_global = total_input_steps_per_epoch * FLAGS.train_epochs\n",
    "        FLAGS.train_steps = total_input_steps_global\n",
    "\n",
    "    if FLAGS.test_steps == 0:\n",
    "        FLAGS.test_steps = int(total_test_records / FLAGS.test_batch_size)\n",
    "\n",
    "    print('Total Train Input Bytes: %d' % (total_input_size))\n",
    "    print('Total Train Input Records: %d' % (total_input_records))\n",
    "    print('Using Train Batch Size: %d' % (FLAGS.train_batch_size))\n",
    "    print('Using Train Epochs: %d' % (FLAGS.train_epochs))\n",
    "    \n",
    "    print('Using Train Steps Per Epoch: %d' % (total_input_steps_per_epoch))\n",
    "    print('Using Train Steps Global: %d' % (total_input_steps_global))\n",
    "    print('Using Test Steps Global: %d' % (FLAGS.test_steps))\n",
    "\n",
    "    print('Using SparkMode: %d' % (FLAGS.spark_mode))\n",
    "    \n",
    "    if FLAGS.spark_mode == 1:\n",
    "        global sc\n",
    "        sc = SparkContext(conf=SparkConf().setAppName(FLAGS.app_name))\n",
    "    \n",
    "        num_executors_str = sc._conf.get(\"spark.executor.instances\")\n",
    "        num_executors = int(3 if not num_executors_str else num_executors_str)\n",
    "        \n",
    "        num_ps = FLAGS.num_ps\n",
    "        print('Total Executors: %d' % num_executors)\n",
    "        print('Total PS: %d' % num_ps)\n",
    "    \n",
    "        tf_cluster = TFCluster.run(sc, main_spark, [sys.argv[0]] + unparsed, num_executors, num_ps, False, input_mode=TFCluster.InputMode.TENSORFLOW)\n",
    "        tf_cluster.shutdown()\n",
    "    else:\n",
    "        main_local()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
